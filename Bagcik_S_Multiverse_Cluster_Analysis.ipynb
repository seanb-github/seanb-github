{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c19031",
   "metadata": {},
   "source": [
    "# Author: S.R.E.A Bagcik\n",
    "### E-mail: s.r.e.a.bagcik@lumc.nl / seanbagcik@hotmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0092f",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "* [Packages](#chapter1)\n",
    "* [Data Wrangling](#chapter2)\n",
    "* [Silhouette Analysis with Euclidean Distance](#chapter3)\n",
    "* [Silhouette Analysis with Gower's Distance](#chapter4)\n",
    "* [The Gap Statistic with Euclidean Distance](#chapter5)\n",
    "    * [Plotting the Gap Statistic](#section_5_1)\n",
    "* [The Gap Statistic with Gower's Distance](#chapter6)    \n",
    "* [Stability Function: Rand & adjusted Rand Index](#chapter7)\n",
    "* [Scenario 1](#chapter8)\n",
    "* [Scenario 1: Prognostic Value & Stability Bootstrap](#chapter9)\n",
    "* [Scenario 1: SHAP Analysis](#chapter10)\n",
    "* [Grid Search Gaussian Mixture](#chapter11)\n",
    "* [Scenario 2](#chapter12)\n",
    "* [Scenario 2: Prognostic Value & Stability Bootstrap](#chapter13)\n",
    "* [Scenario 2: SHAP Analysis](#chapter14) \n",
    "* [Agreement: Pair-Confusion Matrix](#chapter15)\n",
    "* [Agreement: Contingency Table](#chapter16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d4c7f",
   "metadata": {},
   "source": [
    "## Packages <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from typing import Callable, Union\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sn\n",
    "import shap\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "import gower\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.stats import sem\n",
    "from sklearn import metrics, mixture\n",
    "from sklearn.cluster import (\n",
    "    AgglomerativeClustering,\n",
    "    AffinityPropagation,\n",
    "    Birch,\n",
    "    DBSCAN,\n",
    "    KMeans,\n",
    "    MeanShift,\n",
    "    OPTICS,\n",
    "    SpectralClustering\n",
    ")\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    adjusted_mutual_info_score,\n",
    "    log_loss,\n",
    "    mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    "    pairwise_distances,\n",
    "    roc_auc_score,\n",
    "    silhouette_samples,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.metrics.cluster import contingency_matrix, pair_confusion_matrix\n",
    "from sklearn.metrics.cluster._expected_mutual_info_fast import expected_mutual_information\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestCentroid, kneighbors_graph\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler\n",
    "from sklearn.utils import check_random_state, resample\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b6fff",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = ...\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "!jupyter nbconvert --to html --TagRemovePreprocessor.remove_cell_tags={'no'} Bagcik_S_Multiverse_Cluster_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65e499",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d29f1",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = ...\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# Read in data\n",
    "cluster_vars = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_cluster_vars.csv'))\n",
    "\n",
    "gower_matrix = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_gower_matrix.csv'))\n",
    "\n",
    "y = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_GOSE6monthEndpointDerived.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ea746",
   "metadata": {},
   "source": [
    "## Data Wrangling <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "In this code block, we dichotomize the 6-month extended Glasgow Outcome Scale (GOSE). Subsequently, we encode the ordinal categorical feature pupil score. We create dummy variables for the feature injury cause, which is non-ordinal categorical. Last, we scale our features to avoid the scale influencing the Euclidean distance in scenario 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dichotomize\n",
    "y_dich = (y >= 5).astype(int)\n",
    "\n",
    "# Encode \"PupilsBaselineDerived\" (Ordinal categorical)\n",
    "encoder = LabelEncoder()\n",
    "cluster_vars_encoded = cluster_vars.copy()\n",
    "cluster_vars_encoded['InjuryHx.PupilsBaselineDerived'] = encoder.fit_transform(\n",
    "    cluster_vars_encoded['InjuryHx.PupilsBaselineDerived'])\n",
    "\n",
    "# Features for reference distribution in Gap Method\n",
    "cluster_vars_gap = cluster_vars_encoded.copy()\n",
    "cluster_vars_gap['InjuryHx.InjCause'] = encoder.fit_transform(\n",
    "    cluster_vars_encoded['InjuryHx.InjCause'])\n",
    "\n",
    "# One-hot-encoding \"InjuryHx.InjCause\" (Categorical, but equal weights)\n",
    "dummy_columns = pd.get_dummies(cluster_vars_encoded['InjuryHx.InjCause'])\n",
    "cluster_vars_encoded = pd.concat(\n",
    "    [cluster_vars_encoded.drop('InjuryHx.InjCause', axis=1), dummy_columns],\n",
    "    axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "columns_to_scale = [\n",
    "    'Subject.Age', 'InjuryHx.GCSMotorBaselineDerived',\n",
    "    'InjuryHx.GCSScoreBaselineDerived', 'dim1', 'dim2', 'dim3', 'dim4'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_vars_encoded[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "non_scaled_columns = [\n",
    "    col for col in cluster_vars_encoded.columns if col not in columns_to_scale\n",
    "]\n",
    "\n",
    "cluster_vars_encoded_scaled = pd.concat(\n",
    "    [cluster_vars_encoded[non_scaled_columns], scaled_df], axis=1)\n",
    "\n",
    "print(\"Features Used in Cluster Analysis \\n\")\n",
    "for i, column in enumerate(cluster_vars.columns, 1):\n",
    "    print(f\"{i}. {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924a4da",
   "metadata": {},
   "source": [
    "## Silhouette Analysis with Euclidean Distance <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05c904",
   "metadata": {},
   "source": [
    "In this cell we perform Silhouette analysis with Euclidean distance to select the \"optimal\" number of clusters for the three clustering algorithms of Scenario 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23553bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_silhouette_euclidean(algorithm, data, range_n_clusters):\n",
    "\n",
    "    max_silhouette_avg = -1\n",
    "    best_n_clusters = -1\n",
    "    silhouette_values = [\n",
    "    ]  # List to store silhouette scores for each iteration\n",
    "\n",
    "    # Iterate over range of cluster numbers\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Choose clustering algorithm\n",
    "        if algorithm == KMedoids:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  method='alternate').fit(data)\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  linkage='average').fit(data)\n",
    "        elif algorithm == SpectralClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='nearest_neighbors').fit(data)\n",
    "\n",
    "        cluster_labels = clusterer.labels_\n",
    "\n",
    "        # Calculate silhouette score only if there is more than one cluster\n",
    "        if len(set(cluster_labels)) == 1:\n",
    "            silhouette_avg = 0\n",
    "        else:\n",
    "            silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "\n",
    "        # Append the silhouette score\n",
    "        silhouette_values.append(silhouette_avg)\n",
    "\n",
    "        # Update the best silhouette score\n",
    "        if silhouette_avg > max_silhouette_avg:\n",
    "            max_silhouette_avg = silhouette_avg\n",
    "            best_n_clusters = n_clusters\n",
    "\n",
    "    return best_n_clusters, silhouette_values\n",
    "\n",
    "\n",
    "# algorithms = {\n",
    "#     'KMedoids': KMedoids,\n",
    "#     'AgglomerativeClustering': AgglomerativeClustering,\n",
    "#     'SpectralClustering': SpectralClustering\n",
    "# }\n",
    "\n",
    "# for algorithm_name, algorithm in algorithms.items():\n",
    "#     best_n_clusters, silhouette_values = best_silhouette_euclidean(\n",
    "#         algorithm, cluster_vars_encoded, range(2, 30))\n",
    "\n",
    "#     print(\n",
    "#         f\"The maximum silhouette score for {algorithm_name} is {max(silhouette_values)} for n_clusters = {best_n_clusters}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2633a8",
   "metadata": {},
   "source": [
    "## Silhouette Analysis with Gower's distance <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65201c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_silhouette_gower(algorithm, distance, range_n_clusters):\n",
    "    max_silhouette_avg = -1\n",
    "    best_n_clusters = -1\n",
    "    silhouette_values = [\n",
    "    ]  # List to store silhouette scores for each iteration\n",
    "\n",
    "    # Iterate over range of cluster numbers\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Choose clustering algorithm\n",
    "        if algorithm == KMedoids:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  method='alternate',\n",
    "                                  metric='precomputed').fit(distance)\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='precomputed',\n",
    "                                  linkage='average').fit(distance)\n",
    "        elif algorithm == SpectralClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='precomputed').fit(1 - distance)\n",
    "\n",
    "        cluster_labels = clusterer.labels_\n",
    "\n",
    "        # Calculate silhouette score only if there is more than one cluster\n",
    "        if len(set(cluster_labels)) == 1:\n",
    "            silhouette_avg = 0\n",
    "        else:\n",
    "            silhouette_avg = silhouette_score(distance,\n",
    "                                              cluster_labels,\n",
    "                                              metric='precomputed')\n",
    "\n",
    "        # Append silhouette score\n",
    "        silhouette_values.append(silhouette_avg)\n",
    "\n",
    "        # Update the best silhouette score\n",
    "        if silhouette_avg > max_silhouette_avg:\n",
    "            max_silhouette_avg = silhouette_avg\n",
    "            best_n_clusters = n_clusters\n",
    "\n",
    "    return best_n_clusters, silhouette_values\n",
    "\n",
    "# algorithms = {\n",
    "#     'KMedoids': KMedoids,\n",
    "#     'AgglomerativeClustering': AgglomerativeClustering,\n",
    "#     'SpectralClustering': SpectralClustering\n",
    "# }\n",
    "\n",
    "# for algorithm_name, algorithm in algorithms.items():\n",
    "#     best_n_clusters, silhouette_values = best_silhouette_gower(\n",
    "#         algorithm, gower_matrix, range(2, 30))\n",
    "\n",
    "#     print(\n",
    "#         f\"The maximum silhouette score for {algorithm_name} is {max(silhouette_values)} for n_clusters = {best_n_clusters}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f117e",
   "metadata": {},
   "source": [
    "## The Gap Statistic with Euclidean Distance <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439c844",
   "metadata": {},
   "source": [
    "In this cell we perform the Gap method with Euclidean distance to give us the \"optimal\" number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapStatistics:\n",
    "\n",
    "    def __init__(self, return_params: bool = True):\n",
    "        # Constructor for the GapStatistics class\n",
    "        self.return_params = return_params\n",
    "\n",
    "    def _calculate_Wks(self, algorithm, K: int, X: pd.DataFrame) -> list:\n",
    "        # Calculate Wk for clustering algorithm and number of clusters (K)\n",
    "        # Parameters:\n",
    "        # - algorithm: The clustering algorithm to be used\n",
    "        # - K: Number of clusters\n",
    "        # - X: Input data as a pandas DataFrame\n",
    "        # Returns a list of Wks values\n",
    "\n",
    "        # Dummy encoding for categorical columns\n",
    "        dummy_columns = pd.get_dummies(X['InjuryHx.InjCause'])\n",
    "        X = pd.concat([X.drop('InjuryHx.InjCause', axis=1), dummy_columns], axis=1)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        Wks = []\n",
    "\n",
    "        if algorithm == KMedoids:\n",
    "            # KMedoids\n",
    "            for k in np.arange(2, K + 1):\n",
    "                clusterer = algorithm(n_clusters=k, method='alternate').fit(X)\n",
    "                labels = clusterer.predict(X)\n",
    "                centroids = clusterer.cluster_centers_\n",
    "\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            # Agglomerative Clustering\n",
    "            for k in np.arange(2, K + 1):\n",
    "                labels = algorithm(n_clusters=k, linkage='average').fit_predict(X)\n",
    "                tmp = NearestCentroid()\n",
    "                tmp.fit(X, labels)\n",
    "                centroids = tmp.centroids_\n",
    "\n",
    "        elif algorithm == SpectralClustering:\n",
    "            # Spectral Clustering\n",
    "            for k in np.arange(2, K + 1):\n",
    "                labels = algorithm(n_clusters=k, affinity='nearest_neighbors').fit_predict(X)\n",
    "                tmp = NearestCentroid()\n",
    "                tmp.fit(X, labels)\n",
    "                centroids = tmp.centroids_\n",
    "\n",
    "        Ds = []\n",
    "\n",
    "        for i in range(k):\n",
    "            cluster_array = np.array(X[labels == i])\n",
    "            # FORMULA (1)\n",
    "            if len(np.unique(cluster_array)) > 1:\n",
    "                d = pairwise_distances(cluster_array, centroids[i].reshape(1, -1), metric='euclidean')\n",
    "                Ds.append(np.sum(d))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        pooled = 1 / (2 * len(X))\n",
    "        # FORMULA (2)\n",
    "        Wk = np.sum([D * pooled for D in Ds])\n",
    "        Wks.append(Wk)\n",
    "\n",
    "        return Wks\n",
    "\n",
    "    def _simulate_Wks(self, algorithm, X: pd.DataFrame, K: int, n_iterations: int) -> [list, list]:\n",
    "        # Simulate Wks values for multiple iterations by randomizing data.\n",
    "        # Parameters:\n",
    "        # - algorithm: clustering algorithm to be used\n",
    "        # - X: Input data as a pandas DataFrame\n",
    "        # - K: Number of clusters\n",
    "        # - n_iterations: Number of iterations for simulation\n",
    "        # Returns a list of simulated Wks values\n",
    "\n",
    "        cat_columns = [3, 4, 5, 6, 7]\n",
    "        cont_columns = [0, 1, 2, 8, 9, 10, 11]\n",
    "\n",
    "        cat = X.iloc[:, cat_columns]\n",
    "        cont = X.iloc[:, cont_columns]\n",
    "\n",
    "        sampled_X = X.copy()\n",
    "\n",
    "        # Randomly sample categorical and continuous columns\n",
    "        for col in cat.columns:\n",
    "            sampled_X[col] = np.random.randint(low=cat[col].min(), high=cat[col].max(), size=len(X))\n",
    "\n",
    "        for col in cont.columns:\n",
    "            sampled_X[col] = np.random.uniform(low=cont[col].min(), high=cont[col].max(), size=len(X))\n",
    "\n",
    "        simulated_Wks = []\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            # Calculate Wks for each iteration\n",
    "            Wks_star = self._calculate_Wks(algorithm=algorithm, K=K, X=sampled_X)\n",
    "            simulated_Wks.append(Wks_star)\n",
    "\n",
    "        sim_Wks = np.array(simulated_Wks)\n",
    "        return sim_Wks\n",
    "\n",
    "    def fit_predict(self, algorithm, K: int, X: pd.DataFrame, n_iterations: int = 3):\n",
    "        \n",
    "        # Fit the clustering algorithm, calculate Wks, and simulate Wks values\n",
    "        # Parameters:\n",
    "        # - algorithm: The clustering algorithm to be used\n",
    "        # - K: Number of clusters\n",
    "        # - X: Input data as a pandas DataFrame.\n",
    "        # - n_iterations: Number of iterations for simulation\n",
    "        # Returns the optimum number of clusters and additional information if return_params is True\n",
    "\n",
    "        Wks = self._calculate_Wks(algorithm=algorithm, K=K, X=X)\n",
    "        sim_Wks = self._simulate_Wks(algorithm=algorithm, K=K, X=X, n_iterations=n_iterations)\n",
    "\n",
    "        log_Wks = np.log(Wks)\n",
    "        log_Wks_star = np.log(sim_Wks)\n",
    "\n",
    "        sd_k = np.std(log_Wks_star, axis=0)\n",
    "        sim_sks = np.sqrt(1 + (1 / n_iterations)) * sd_k\n",
    "\n",
    "        gaps = np.mean(log_Wks_star - log_Wks, axis=0)\n",
    "\n",
    "        optimum = 1\n",
    "        max_gap = gaps[0]\n",
    "\n",
    "        # GAP - FORMULA (3)\n",
    "        for i in range(0, len(gaps) - 1):\n",
    "            if gaps[i] >= gaps[i + 1] - sim_sks[i + 1]:\n",
    "                if gaps[i] > max_gap:\n",
    "                    optimum = i\n",
    "                    max_gap = gaps[i]\n",
    "\n",
    "        self.params = {\n",
    "            'Wks': Wks,\n",
    "            'sim_Wks': sim_Wks,\n",
    "            'sim_sks': sim_sks,\n",
    "            'gaps': gaps\n",
    "        }\n",
    "\n",
    "        if self.return_params:\n",
    "            return optimum, self.params\n",
    "        else:\n",
    "            return optimum\n",
    "\n",
    "\n",
    "GapStatEucl = GapStatistics(return_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c13009",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Plotting the Gap Statistic <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfb138",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "    optimum, params = GapStatEucl.fit_predict(algorithm,\n",
    "                                              K=15,\n",
    "                                              X=cluster_vars_gap)\n",
    "\n",
    "Wks = GapStatEucl.params['Wks']\n",
    "sim_Wks = GapStatEucl.params['sim_Wks']\n",
    "sim_sks = GapStatEucl.params['sim_sks']\n",
    "gaps = GapStatEucl.params['gaps']\n",
    "\n",
    "log_Wks = np.log(Wks)\n",
    "log_sim_Wks = np.log(np.mean(sim_Wks, axis=0))\n",
    "Wks = Wks - np.max(Wks)\n",
    "\n",
    "\n",
    "def clip_values(arr):\n",
    "    \"\"\"\n",
    "    Helper function to clip values in the array between 0 and 1.\n",
    "    \"\"\"\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = max(0, min(1, arr[i]))\n",
    "    return arr\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Bottom right\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(Wks, '-o', label=\"Wks from Data\")\n",
    "plt.plot(log_Wks, '-o', label=\"Logged Wks from Data\")\n",
    "plt.plot(log_sim_Wks, '-o', color=\"green\", label=f\"Logged Simulated Wks\")\n",
    "plt.title(\"Decrease of Within Cluster Distance\")\n",
    "plt.legend()\n",
    "\n",
    "# Bottom left\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(gaps, '-o', color='r')\n",
    "\n",
    "# Normalize yminx and ymaxsx to be between 0 and 1\n",
    "yminx = ((gaps - sim_sks) - np.min(gaps)) / (np.max(gaps) - np.min(gaps))\n",
    "ymaxsx = ((gaps + sim_sks) - np.min(gaps)) / (np.max(gaps) - np.min(gaps))\n",
    "\n",
    "# Clip values to ensure they are between 0 and 1\n",
    "clipped_yminx = clip_values(yminx)\n",
    "clipped_ymaxsx = clip_values(ymaxsx)\n",
    "\n",
    "# Plot vertical lines with clipped ymin and ymax\n",
    "for i in range(len(gaps)):\n",
    "    plt.axvline(x=i,\n",
    "                ymin=clipped_yminx[i],\n",
    "                ymax=clipped_ymaxsx[i],\n",
    "                color='black')\n",
    "\n",
    "plt.axvline(x=optimum, color='green')\n",
    "plt.title(\"Gap Statistics with optimum K at {}\".format(optimum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524895c",
   "metadata": {},
   "source": [
    "## The Gap Statistic with Gower's Distance <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1e642",
   "metadata": {},
   "source": [
    "In this cell we perform the Gap method with Gower's distance to give us the \"optimal\" number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapStatisticsGower:\n",
    "\n",
    "    def __init__(self, return_params: bool = True):\n",
    "        # Constructor for the GapStatistics class\n",
    "        self.return_params = return_params\n",
    "\n",
    "    def _calculate_Wks(self, algorithm, K: int, X: pd.DataFrame) -> list:\n",
    "        # Calculate Wk for clustering algorithm and number of clusters (K)\n",
    "        # Parameters:\n",
    "        # - algorithm: The clustering algorithm to be used\n",
    "        # - K: Number of clusters\n",
    "        # - X: Input data as a pandas DataFrame\n",
    "        # Returns a list of Wks values\n",
    "\n",
    "        # Dummy encoding for categorical columns\n",
    "        dummy_columns = pd.get_dummies(X['InjuryHx.InjCause'])\n",
    "        X = pd.concat([X.drop('InjuryHx.InjCause', axis=1), dummy_columns], axis=1)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        Wks = []\n",
    "\n",
    "        for k in np.arange(2, K + 1):\n",
    "            if algorithm == KMedoids:\n",
    "                clusterer = algorithm(n_clusters=k, method='alternate').fit(X)\n",
    "                labels = clusterer.predict(X)        \n",
    "            elif algorithm == AgglomerativeClustering:\n",
    "                labels = AgglomerativeClustering(n_clusters=k, linkage='average').fit_predict(X)\n",
    "            elif algorithm == SpectralClustering:\n",
    "                labels = SpectralClustering(n_clusters=k, affinity='nearest_neighbors').fit_predict(X)\n",
    "\n",
    "            X['labels'] = labels\n",
    "\n",
    "            Ds = []\n",
    "\n",
    "            for label in np.unique(X['labels']):\n",
    "                data = X[X['labels'] == label].drop(columns=['labels'])\n",
    "                n_k = len(data)\n",
    "                pooled = 1 / (2 * n_k)\n",
    "                d = np.sum(gower.gower_matrix(data))\n",
    "\n",
    "                Ds.append(d)\n",
    "\n",
    "            Wk = np.sum([D * pooled for D in Ds])\n",
    "            Wks.append(Wk)\n",
    "\n",
    "            X.drop(columns=['labels'], inplace=True)\n",
    "\n",
    "        return Wks\n",
    "\n",
    "    def _simulate_Wks(self, algorithm, X: pd.DataFrame, K: int, n_iterations: int) -> [list, list]:\n",
    "        # Simulate Wks values for multiple iterations by randomizing data.\n",
    "        # Parameters:\n",
    "        # - algorithm: clustering algorithm to be used\n",
    "        # - X: Input data as a pandas DataFrame\n",
    "        # - K: Number of clusters\n",
    "        # - n_iterations: Number of iterations for simulation\n",
    "        # Returns a list of simulated Wks values\n",
    "\n",
    "        cat_columns = [3, 4, 5, 6, 7]\n",
    "        cont_columns = [0, 1, 2, 8, 9, 10, 11]\n",
    "\n",
    "        cat = X.iloc[:, cat_columns]\n",
    "        cont = X.iloc[:, cont_columns]\n",
    "\n",
    "        sampled_X = X.copy()\n",
    "\n",
    "        # Randomly sample categorical and continuous columns\n",
    "        for col in cat.columns:\n",
    "            sampled_X[col] = np.random.randint(low=cat[col].min(), high=cat[col].max(), size=len(X))\n",
    "\n",
    "        for col in cont.columns:\n",
    "            sampled_X[col] = np.random.uniform(low=cont[col].min(), high=cont[col].max(), size=len(X))\n",
    "\n",
    "        simulated_Wks = []\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            # Calculate Wks for each iteration\n",
    "            Wks_star = self._calculate_Wks(algorithm=algorithm, K=K, X=sampled_X)\n",
    "            simulated_Wks.append(Wks_star)\n",
    "\n",
    "        sim_Wks = np.array(simulated_Wks)\n",
    "        return sim_Wks\n",
    "\n",
    "    def fit_predict(self, algorithm, K: int, X: pd.DataFrame, n_iterations: int = 3):\n",
    "        \n",
    "        # Fit the clustering algorithm, calculate Wks, and simulate Wks values\n",
    "        # Parameters:\n",
    "        # - algorithm: The clustering algorithm to be used\n",
    "        # - K: Number of clusters\n",
    "        # - X: Input data as a pandas DataFrame.\n",
    "        # - n_iterations: Number of iterations for simulation\n",
    "        # Returns the optimum number of clusters and additional information if return_params is True\n",
    "\n",
    "        Wks = self._calculate_Wks(algorithm=algorithm, K=K, X=X)\n",
    "        sim_Wks = self._simulate_Wks(algorithm=algorithm, K=K, X=X, n_iterations=n_iterations)\n",
    "\n",
    "        log_Wks = np.log(Wks)\n",
    "        log_Wks_star = np.log(sim_Wks)\n",
    "\n",
    "        sd_k = np.std(log_Wks_star, axis=0)\n",
    "        sim_sks = np.sqrt(1 + (1 / n_iterations)) * sd_k\n",
    "\n",
    "        gaps = np.mean(log_Wks_star - log_Wks, axis=0)\n",
    "\n",
    "        optimum = 1\n",
    "        max_gap = gaps[0]\n",
    "\n",
    "        # GAP - FORMULA (3)\n",
    "        for i in range(0, len(gaps) - 1):\n",
    "            if gaps[i] >= gaps[i + 1] - sim_sks[i + 1]:\n",
    "                if gaps[i] > max_gap:\n",
    "                    optimum = i\n",
    "                    max_gap = gaps[i]\n",
    "\n",
    "        self.params = {\n",
    "            'Wks': Wks,\n",
    "            'sim_Wks': sim_Wks,\n",
    "            'sim_sks': sim_sks,\n",
    "            'gaps': gaps\n",
    "        }\n",
    "\n",
    "        if self.return_params:\n",
    "            return optimum, self.params\n",
    "        else:\n",
    "            return optimum\n",
    "\n",
    "GapStatGow = GapStatisticsGower(return_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac93ce1",
   "metadata": {},
   "source": [
    "## Stability Function: Rand & adjusted Rand Index <a class=\"anchor\" id=\"chapter7\"></a>\n",
    "\n",
    "This function calculates the stability of a clustering method given a number of bootstraps $B=200$ quantified via the Rand and adjusted Rand index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e20094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_bootstrap(labels, n_bootstraps):\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for i in range(0, n_bootstraps - 1):\n",
    "        for j in range(i + 1, n_bootstraps):\n",
    "\n",
    "            pairconf = pair_confusion_matrix(labels.iloc[:, i], labels.iloc[:,j])\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) * (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67737a5",
   "metadata": {},
   "source": [
    "## Scenario 1 <a class=\"anchor\" id=\"chapter8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29b5d8",
   "metadata": {},
   "source": [
    "In this cell we prepare the clustering algorithms of Scenario 1. The hard-coded values imply the \"optimal\" number of clusters give one of the four cluster selection methods: Silhouette with Euclidean distance, Silhouette with Gower's distance, the Gap method with Euclidean distance and the Gap method with Gower's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfaa9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_algorithms():\n",
    "    kmedoids_sil_G12 = KMedoids(n_clusters=12,\n",
    "                                metric='precomputed',\n",
    "                                method='pam')\n",
    "    kmedoids_sil_E2 = KMedoids(n_clusters=2, method='pam')\n",
    "    agglomerative_sil_G2 = AgglomerativeClustering(n_clusters=2,\n",
    "                                                   affinity='precomputed',\n",
    "                                                   linkage='average')\n",
    "    agglomerative_sil_E2 = AgglomerativeClustering(n_clusters=2,\n",
    "                                                   linkage='average')\n",
    "    spectral_sil_G2 = SpectralClustering(n_clusters=2, affinity='precomputed')\n",
    "    spectral_sil_E2 = SpectralClustering(n_clusters=2,\n",
    "                                         affinity='nearest_neighbors')\n",
    "    kmedoids_gap_G15 = KMedoids(n_clusters=15,\n",
    "                                metric='precomputed',\n",
    "                                method='pam')\n",
    "    kmedoids_gap_E14 = KMedoids(n_clusters=14, method='pam')\n",
    "    agglomerative_gap_G6 = AgglomerativeClustering(n_clusters=6,\n",
    "                                                   affinity='precomputed',\n",
    "                                                   linkage='average')\n",
    "\n",
    "    agglomerative_gap_E25 = AgglomerativeClustering(n_clusters=25,\n",
    "                                                    linkage='average')\n",
    "    spectral_gap_G7 = SpectralClustering(n_clusters=7, affinity='precomputed')\n",
    "    spectral_gap_E17 = SpectralClustering(n_clusters=17,\n",
    "                                          affinity='nearest_neighbors')\n",
    "\n",
    "    return [('KM-Gow-Sil', kmedoids_sil_G12), ('KM-Eucl-Sil', kmedoids_sil_E2),\n",
    "            ('AC-Gow-Sil', agglomerative_sil_G2),\n",
    "            ('AC-Eucl-Sil', agglomerative_sil_E2),\n",
    "            ('SC-Gow-Sil', spectral_sil_G2), ('SC-Eucl-Sil', spectral_sil_E2),\n",
    "            ('KM-Gow-Gap', kmedoids_gap_G15),\n",
    "            ('KM-Eucl-Gap', kmedoids_gap_E14),\n",
    "            ('AC-Gow-Gap', agglomerative_gap_G6),\n",
    "            ('AC-Eucl-Gap', agglomerative_gap_E25),\n",
    "            ('SC-Gow-Gap', spectral_gap_G7), ('SC-Eucl-Gap', spectral_gap_E17)]\n",
    "\n",
    "clustering_algorithms = prepare_algorithms()\n",
    "\n",
    "def get_labels(X, X_distance, X_similarity):\n",
    "    labels = {}\n",
    "\n",
    "    for algorithm_name, algorithm in clustering_algorithms:\n",
    "        try:\n",
    "            if algorithm_name.startswith('SC') and algorithm_name.endswith(\n",
    "                ('Gow-Sil', 'Gow-Gap')):\n",
    "                algorithm.fit(X_similarity)\n",
    "            elif algorithm_name.startswith(\n",
    "                ('KM', 'AC')) and algorithm_name.endswith(\n",
    "                    ('Gow-Sil', 'Gow-Gap')):\n",
    "                algorithm.fit(X_distance)\n",
    "            else:\n",
    "                algorithm.fit(X)\n",
    "\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels[algorithm_name] = algorithm.labels_.astype(int)\n",
    "            else:\n",
    "                raise AttributeError(\n",
    "                    f\"{algorithm_name} does not have a labels_ attribute.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {algorithm_name} could not be fitted. {e}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "cluster_labels = get_labels(cluster_vars_encoded, gower_matrix, 1 - gower_matrix)\n",
    "\n",
    "def print_label_counts(cluster_labels, X, X_distance):\n",
    "    if not cluster_labels:\n",
    "        print(\"Error: Input is empty.\")\n",
    "        return\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        label_counts = Counter(labels)\n",
    "        cluster_sizes = list(label_counts.values())\n",
    "\n",
    "        print(f\"Algorithm: {algorithm_name}\")\n",
    "        print(f\"Number of unique labels: {len(label_counts)}\")\n",
    "        print(f\"Smallest: {min(cluster_sizes)}\")\n",
    "        print(f\"Largest: {max(cluster_sizes)}\")\n",
    "        print(f\"Average: {np.mean(cluster_sizes)}\")\n",
    "        print(f\"Median: {np.median(cluster_sizes)}\")\n",
    "\n",
    "        if len(label_counts) == 1:\n",
    "            print(\"Silhouette score: Not applicable (only one cluster)\\n\")\n",
    "            continue\n",
    "        if algorithm_name.endswith('Gap'):\n",
    "            print(\"Silhouette score: Not applicable (Gap Method)\\n\")\n",
    "            continue\n",
    "        if algorithm_name.endswith('Gow-Sil'):\n",
    "            silhouette = silhouette_score(X_distance,\n",
    "                                          labels,\n",
    "                                          metric='precomputed')\n",
    "            print(f\"Silhouette score: {silhouette}\\n\")\n",
    "        else:\n",
    "            silhouette = silhouette_score(X, labels, metric='euclidean')\n",
    "            print(f\"Silhouette score: {silhouette}\\n\")\n",
    "\n",
    "print_label_counts(cluster_labels, cluster_vars_encoded, gower_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de36b7f",
   "metadata": {},
   "source": [
    "## Scenario 1: Prognostic Value & Stability Bootstrap <a class=\"anchor\" id=\"chapter9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f71ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read cluster labels from CSV file\n",
    "cluster_labels = pd.read_csv(os.path.join(DATA_DIR, 'cluster_labels_s1.csv'))\n",
    "\n",
    "# Combine target variable (y_dich) with encoded cluster variables\n",
    "orig_data = pd.concat([y_dich, cluster_vars_encoded], axis=1)\n",
    "orig_data.columns = ['y_orig'] + list(cluster_vars_encoded.columns)\n",
    "\n",
    "# Combine target variable (y_dich) with gap-encoded cluster variables\n",
    "orig_data_gap = pd.concat([y_dich, cluster_vars_gap], axis=1)\n",
    "orig_data_gap.columns = ['y_orig'] + list(cluster_vars_gap.columns)\n",
    "\n",
    "# Set bootstrap iterations, range of clusters, fixed number of clusters (k)\n",
    "n_bootstraps = 10\n",
    "range_n_clusters = range(2, 15)\n",
    "K = 8\n",
    "\n",
    "# Lists to store AUC values and cluster labels\n",
    "aucs_boot_eucl_sil, aucs_boot_eucl_gap, aucs_boot_gow_sil, aucs_boot_gow_gap, clusters = [], [], [], [], []\n",
    "\n",
    "# DataFrame to store AUC Apparent values for each algorithm\n",
    "auc_apparents = pd.DataFrame(columns=['method', 'auc_apparent'])\n",
    "\n",
    "# Dictionary of clustering algorithms\n",
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "# Iterate through clustering algorithms and calculate AUC Apparent\n",
    "for algorithm, labels in cluster_labels.items():\n",
    "    # Create DataFrame for logistic regression with cluster labels\n",
    "    df_orig = pd.DataFrame({\"y_orig\": np.ravel(y_dich), \"labels_orig\": np.ravel(labels)})\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = sm.formula.glm(formula=\"y_orig ~ labels_orig\", family=sm.families.Binomial(), data=df_orig).fit()\n",
    "\n",
    "    # Calculate AUC Apparent\n",
    "    y_pred_apparent = model.predict(df_orig['labels_orig'])\n",
    "    auc_apparent = roc_auc_score(df_orig['y_orig'], y_pred_apparent)\n",
    "\n",
    "    # Append results to auc_apparents DataFrame\n",
    "    auc_apparents = auc_apparents.append(\n",
    "        {\n",
    "            'method': algorithm,\n",
    "            'auc_apparent': auc_apparent\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display AUC Apparent values\n",
    "print(auc_apparents)\n",
    "print()\n",
    "\n",
    "# Iterate through clustering algorithms and perform bootstrap iterations\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "    optimisms_eucl_sil, optimisms_eucl_gap, optimisms_gow_sil, optimisms_gow_gap = [], [], [], []\n",
    "    ls_orig_eucl_sil_dfs, ls_orig_eucl_gap_dfs, ls_orig_gow_sil_dfs, ls_orig_gow_gap_dfs = [], [], [], []\n",
    "\n",
    "    # Perform bootstrap iterations\n",
    "    for i in range(n_bootstraps):\n",
    "        sample = resample(orig_data, replace=True, n_samples=4509)\n",
    "        sample_gap = resample(orig_data_gap, replace=True, n_samples=4509)\n",
    "        y_boot = sample['y_orig']\n",
    "        gow = gower.gower_matrix(sample.iloc[:, 1:])\n",
    "                                 \n",
    "        n_eucl_sil = best_silhouette_euclidean(algorithm, sample.iloc[:, 1:], range_n_clusters)[0]\n",
    "        n_eucl_gap = GapStatEucl.fit_predict(algorithm, K, sample_gap.iloc[:,1:])\n",
    "        n_gow_sil = best_silhouette_gower(algorithm, gow, range_n_clusters)[0]\n",
    "        n_gow_gap = GapStatGow.fit_predict(algorithm, K, sample_gap.iloc[:, 1:])\n",
    "\n",
    "        # (2) AUC Bootstrap\n",
    "        if algorithm_name == 'KMedoids':        \n",
    "            clusterer_eucl_sil = KMedoids(n_clusters=n_eucl_sil).fit(sample.iloc[:, 1:])\n",
    "            clusterer_eucl_gap = KMedoids(n_clusters=n_eucl_gap).fit(sample.iloc[:, 1:])\n",
    "            clusterer_gow_sil = KMedoids(n_clusters=n_gow_sil, metric='precomputed').fit(gow)\n",
    "            clusterer_gow_gap = KMedoids(n_clusters=n_gow_gap, metric='precomputed').fit(gow)\n",
    "\n",
    "            l_orig_eucl_sil = pd.DataFrame(clusterer_eucl_sil.predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_eucl_gap = pd.DataFrame(clusterer_eucl_gap.predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_gow_sil = pd.DataFrame(clusterer_gow_sil.predict(gower_matrix))\n",
    "            l_orig_gow_gap = pd.DataFrame(clusterer_gow_gap.predict(gower_matrix))\n",
    "\n",
    "        elif algorithm_name == 'AgglomerativeClustering':\n",
    "            clusterer_eucl_sil = AgglomerativeClustering(n_clusters=n_eucl_sil,linkage='average').fit(sample.iloc[:, 1:])\n",
    "            clusterer_eucl_gap = AgglomerativeClustering(n_clusters=n_eucl_gap,linkage='average').fit(sample.iloc[:, 1:])\n",
    "            clusterer_gow_sil = AgglomerativeClustering(n_clusters=n_gow_sil, metric='precomputed', linkage='average').fit(gow)\n",
    "            clusterer_gow_gap = AgglomerativeClustering(n_clusters=n_gow_gap, metric='precomputed', linkage='average').fit(gow)\n",
    "\n",
    "            l_orig_eucl_sil = pd.DataFrame(clusterer_eucl_sil.fit_predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_eucl_gap = pd.DataFrame(clusterer_eucl_gap.fit_predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_gow_sil = pd.DataFrame(clusterer_gow_sil.fit_predict(gower_matrix))\n",
    "            l_orig_gow_gap = pd.DataFrame(clusterer_gow_gap.fit_predict(gower_matrix))\n",
    "\n",
    "        elif algorithm_name == 'SpectralClustering':\n",
    "            clusterer_eucl_sil = SpectralClustering(n_clusters=n_eucl_sil, affinity='nearest_neighbors').fit(sample.iloc[:, 1:])\n",
    "            clusterer_eucl_gap = SpectralClustering(n_clusters=n_eucl_gap, affinity='nearest_neighbors').fit(sample.iloc[:, 1:])\n",
    "            clusterer_gow_sil = SpectralClustering(n_clusters=n_gow_sil, affinity='precomputed').fit(1 - gow)\n",
    "            clusterer_gow_gap = SpectralClustering(n_clusters=n_gow_gap, affinity='precomputed').fit(1 - gow)\n",
    "\n",
    "            l_orig_eucl_sil = pd.DataFrame(clusterer_eucl_sil.fit_predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_eucl_gap = pd.DataFrame(clusterer_eucl_gap.fit_predict(orig_data.iloc[:, 1:]))\n",
    "            l_orig_gow_sil = pd.DataFrame(clusterer_gow_sil.fit_predict(gower_matrix))\n",
    "            l_orig_gow_gap = pd.DataFrame(clusterer_gow_gap.fit_predict(gower_matrix))\n",
    "\n",
    "        l_boot_eucl_sil = pd.DataFrame(clusterer_eucl_sil.labels_)\n",
    "        l_boot_eucl_gap = pd.DataFrame(clusterer_eucl_gap.labels_)\n",
    "        l_boot_gow_sil = pd.DataFrame(clusterer_gow_sil.labels_)\n",
    "        l_boot_gow_gap = pd.DataFrame(clusterer_gow_gap.labels_)\n",
    "\n",
    "        df_boot_eucl_sil = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"l_boot_eucl_sil\": np.ravel(l_boot_eucl_sil)})\n",
    "        df_boot_eucl_gap = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"l_boot_eucl_gap\": np.ravel(l_boot_eucl_gap)})\n",
    "        df_boot_gow_sil = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"l_boot_gow_sil\": np.ravel(l_boot_gow_sil)})\n",
    "        df_boot_gow_gap = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"l_boot_gow_gap\": np.ravel(l_boot_gow_gap)})\n",
    "\n",
    "        m_eucl_sil = sm.formula.glm(formula=\"y_boot ~ l_boot_eucl_sil\", family=sm.families.Binomial(), data=df_boot_eucl_sil).fit()\n",
    "        m_eucl_gap = sm.formula.glm(formula=\"y_boot ~ l_boot_eucl_gap\", family=sm.families.Binomial(), data=df_boot_eucl_gap).fit()\n",
    "        m_gow_sil = sm.formula.glm(formula=\"y_boot ~ l_boot_gow_sil\", family=sm.families.Binomial(), data=df_boot_gow_sil).fit()\n",
    "        m_gow_gap = sm.formula.glm(formula=\"y_boot ~ l_boot_gow_gap\", family=sm.families.Binomial(), data=df_boot_gow_gap).fit()\n",
    "\n",
    "        # Calculate AUC Bootstrap\n",
    "        models = [m_eucl_sil, m_eucl_gap, m_gow_sil, m_gow_gap]\n",
    "        label_dfs = [l_boot_eucl_sil, l_boot_eucl_gap, l_boot_gow_sil, l_boot_gow_gap]\n",
    "        auc_scores = []\n",
    "\n",
    "        # Iterate over models and label dataframes\n",
    "        for model, label_df in zip(models, label_dfs):\n",
    "            # Predictions\n",
    "            y_pred_boot = model.predict(label_df)\n",
    "\n",
    "            # AUC score\n",
    "            auc_boot = roc_auc_score(y_boot, y_pred_boot)\n",
    "\n",
    "            # Append to the list of AUC scores\n",
    "            auc_scores.append(auc_boot)\n",
    "\n",
    "        # Separate the AUC scores for each algorithm\n",
    "        auc_boot_eucl_sil, auc_boot_eucl_gap, auc_boot_gow_sil, auc_boot_gow_gap = [auc_scores[i] for i in range(4)]\n",
    "\n",
    "        # Append to the respective lists\n",
    "        aucs_boot_eucl_sil.append(auc_boot_eucl_sil)\n",
    "        aucs_boot_eucl_gap.append(auc_boot_eucl_gap)\n",
    "        aucs_boot_gow_sil.append(auc_boot_gow_sil)\n",
    "        aucs_boot_gow_gap.append(auc_boot_gow_gap)\n",
    "\n",
    "        # Calculate AUC Original\n",
    "        y_pred_orig_eucl_sil = m_eucl_sil.predict(pd.DataFrame(df_orig['labels_orig']))\n",
    "        y_pred_orig_eucl_gap = m_eucl_gap.predict(pd.DataFrame(df_orig['labels_orig']))\n",
    "        y_pred_orig_gow_sil = m_gow_sil.predict(pd.DataFrame(df_orig['labels_orig']))\n",
    "        y_pred_orig_gow_gap = m_gow_gap.predict(pd.DataFrame(df_orig['labels_orig']))\n",
    "\n",
    "        auc_orig_eucl_sil = roc_auc_score(df_orig['y_orig'], y_pred_orig_eucl_sil)\n",
    "        auc_orig_eucl_gap = roc_auc_score(df_orig['y_orig'], y_pred_orig_eucl_gap)\n",
    "        auc_orig_gow_sil = roc_auc_score(df_orig['y_orig'], y_pred_orig_gow_sil)\n",
    "        auc_orig_gow_gap = roc_auc_score(df_orig['y_orig'], y_pred_orig_gow_gap)\n",
    "\n",
    "        # Calculate optimism and append results\n",
    "        optimism_eucl_sil = auc_boot_eucl_sil - auc_orig_eucl_sil\n",
    "        optimism_eucl_gap = auc_boot_eucl_gap - auc_orig_eucl_gap\n",
    "        optimism_gow_sil = auc_boot_gow_sil - auc_orig_gow_sil\n",
    "        optimism_gow_gap = auc_boot_gow_gap - auc_orig_gow_gap\n",
    "\n",
    "        optimisms_eucl_sil.append(optimism_eucl_sil)\n",
    "        optimisms_eucl_gap.append(optimism_eucl_gap)\n",
    "        optimisms_gow_sil.append(optimism_gow_sil)\n",
    "        optimisms_gow_gap.append(optimism_gow_gap)\n",
    "\n",
    "        ls_orig_eucl_sil_dfs.append(l_orig_eucl_sil.reset_index(drop=True))\n",
    "        ls_orig_eucl_gap_dfs.append(l_orig_eucl_gap.reset_index(drop=True))\n",
    "        ls_orig_gow_sil_dfs.append(l_orig_gow_sil.reset_index(drop=True))\n",
    "        ls_orig_gow_gap_dfs.append(l_orig_gow_gap.reset_index(drop=True))\n",
    "\n",
    "    # Calculate average optimism\n",
    "    optimism_eucl_sil_avg = np.sum(optimisms_eucl_sil) / n_bootstraps\n",
    "    optimism_eucl_gap_avg = np.sum(optimisms_eucl_gap) / n_bootstraps\n",
    "    optimism_gow_sil_avg = np.sum(optimisms_gow_sil) / n_bootstraps\n",
    "    optimism_gow_gap_avg = np.sum(optimisms_gow_gap) / n_bootstraps\n",
    "\n",
    "    std_auc_boot_eucl_sil = np.std(aucs_boot_eucl_sil)\n",
    "    std_auc_boot_eucl_gap = np.std(aucs_boot_eucl_gap)\n",
    "    std_auc_boot_gow_sil = np.std(aucs_boot_gow_sil)\n",
    "    std_auc_boot_gow_gap = np.std(aucs_boot_gow_gap)\n",
    "            \n",
    "    print(f\"Algorithm: {algorithm_name}\\n\"\n",
    "          f\"Optimism Metrics:\\n\"\n",
    "          f\"  - Euclidean Silhouette: {optimism_eucl_sil_avg}\\n\"\n",
    "          f\"  - Euclidean Gap: {optimism_eucl_gap_avg}\\n\"\n",
    "          f\"  - Gower Silhouette: {optimism_gow_sil_avg}\\n\"\n",
    "          f\"  - Gower Gap: {optimism_gow_gap_avg}\\n\"\n",
    "\n",
    "          f\"Standard Deviation of Bootstrap AUCs:\\n\"\n",
    "          f\"  - Euclidean Silhouette: {std_auc_boot_eucl_sil}\\n\"\n",
    "          f\"  - Euclidean Gap: {std_auc_boot_eucl_gap}\\n\"\n",
    "          f\"  - Gower Silhouette: {std_auc_boot_eucl_gap}\\n\"\n",
    "          f\"  - Gower Gap: {std_auc_boot_gow_gap}\\n\")\n",
    "            \n",
    "    #         auc_o = auc_apparent - optimism_avg\n",
    "\n",
    "    #         ci = [\n",
    "    #             auc_o - 1.96 * np.std(aucs_bootstrap),\n",
    "    #             auc_o + 1.96 * np.std(aucs_bootstrap)\n",
    "    #         ]\n",
    "\n",
    "    # Stability\n",
    "\n",
    "#     stability = stability_bootstrap(ls_orig_eucl_sil, n_bootstraps)\n",
    "\n",
    "#     print(stability)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ff319",
   "metadata": {},
   "source": [
    "## S1 Bootstrap Avoiding DRY-er code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01507a81",
   "metadata": {},
   "source": [
    "In the cell above the code contains repeated lines. \"Don't repeat yourself\" (DRY) is a principle of software development aimed at reducing such repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87004a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_n(method, algorithm, smpl, smpl_gap, range_n_clusters):\n",
    "    \n",
    "    d = {\n",
    "        \"eucl_sil\": (best_silhouette_euclidean, (algorithm, smpl, range_n_clusters)),\n",
    "        \"eucl_gap\": (GapStatEucl.fit_predict, (algorithm, K, smpl_gap)),\n",
    "        \"gow_sil\": (best_silhouette_gower, (algorithm, gower.gower_matrix(smpl), range_n_clusters)),\n",
    "        \"gow_gap\": (GapStatGow.fit_predict, (algorithm, K, pd.DataFrame(smpl_gap)))\n",
    "    }\n",
    "\n",
    "    return d[method]\n",
    "\n",
    "def auc_bootstrap(algo_name, method, n, sample_iloc, orig_iloc):\n",
    "    if algo_name == 'KMedoids':  \n",
    "        clusterer = (\n",
    "            KMedoids(n_clusters=n).fit(sample_iloc)\n",
    "            if method.startswith(\"eucl\") \n",
    "            else KMedoids(n_clusters=n, metric='precomputed').fit(gower.gower_matrix(sample_iloc))\n",
    "        )\n",
    "\n",
    "        l_orig = pd.DataFrame(clusterer.predict(orig_iloc))\n",
    "\n",
    "\n",
    "    elif algo_name == 'AgglomerativeClustering':\n",
    "        clusterer = (\n",
    "            AgglomerativeClustering(n_clusters=n, linkage='average').fit(sample_iloc)\n",
    "            if method.startswith(\"eucl\") \n",
    "            else AgglomerativeClustering(n_clusters=n, metric='precomputed', linkage='average').fit(gower.gower_matrix(sample_iloc))\n",
    "        )\n",
    "        \n",
    "        l_orig = (\n",
    "            pd.DataFrame(clusterer.fit_predict(orig_data_iloc))\n",
    "            if method.startswith(\"eucl\") \n",
    "            else pd.DataFrame(clusterer.fit_predict(gower_matrix))\n",
    "        )\n",
    "\n",
    "    elif algo_name == 'SpectralClustering':\n",
    "        clusterer = (\n",
    "            SpectralClustering(n_clusters=n, affinity='nearest_neighbors').fit(sample_iloc)\n",
    "            if method.startswith(\"eucl\") \n",
    "            else SpectralClustering(n_clusters=n, affinity='precomputed').fit(1 - gower.gower_matrix(sample_iloc))\n",
    "        )\n",
    "\n",
    "        l_orig = (\n",
    "            pd.DataFrame(clusterer.fit_predict(orig_data_iloc))\n",
    "            if method.startswith(\"eucl\") \n",
    "            else pd.DataFrame(clusterer.fit_predict(gower.gower_matrix(sample_iloc)))\n",
    "        )\n",
    "\n",
    "    return clusterer, l_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4476ae6",
   "metadata": {},
   "source": [
    "## S1 Bootstrap Avoiding DRY-er code [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cluster labels from CSV file\n",
    "cluster_labels = pd.read_csv(os.path.join(DATA_DIR, 'cluster_labels_s1.csv'))\n",
    "\n",
    "# Combine target variable (y_dich) with encoded cluster variables\n",
    "orig_data = pd.concat([y_dich, cluster_vars_encoded], axis=1)\n",
    "orig_data.columns = ['y_orig'] + list(cluster_vars_encoded.columns)\n",
    "\n",
    "# Combine target variable (y_dich) with gap-encoded cluster variables\n",
    "orig_data_gap = pd.concat([y_dich, cluster_vars_gap], axis=1)\n",
    "orig_data_gap.columns = ['y_orig'] + list(cluster_vars_gap.columns)\n",
    "\n",
    "# Set bootstrap iterations, range of clusters, fixed number of clusters (k)\n",
    "n_bootstraps = 3\n",
    "range_n_clusters = range(2, 8)\n",
    "K = 8\n",
    "\n",
    "# Lists to store AUC values and cluster labels\n",
    "aucs_boot_eucl_sil, aucs_boot_eucl_gap, aucs_boot_gow_sil, aucs_boot_gow_gap, clusters = [], [], [], [], []\n",
    "\n",
    "# DataFrame to store AUC Apparent values for each algorithm\n",
    "auc_apparents = pd.DataFrame(columns=['method', 'auc_apparent'])\n",
    "\n",
    "# Dictionary of clustering algorithms\n",
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "# Iterate through clustering algorithms and calculate AUC Apparent\n",
    "for algorithm, labels in cluster_labels.items():\n",
    "    # Create DataFrame for logistic regression with cluster labels\n",
    "    df_orig = pd.DataFrame({\"y_orig\": np.ravel(y_dich), \"labels_orig\": np.ravel(labels)})\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = sm.formula.glm(formula=\"y_orig ~ labels_orig\", family=sm.families.Binomial(), data=df_orig).fit()\n",
    "\n",
    "    # Calculate AUC Apparent\n",
    "    y_pred_apparent = model.predict(df_orig['labels_orig'])\n",
    "    auc_apparent = roc_auc_score(df_orig['y_orig'], y_pred_apparent)\n",
    "\n",
    "    # Append results to auc_apparents DataFrame\n",
    "    auc_apparents = auc_apparents.append(\n",
    "        {\n",
    "            'method': algorithm,\n",
    "            'auc_apparent': auc_apparent\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display AUC Apparent values\n",
    "print(auc_apparents)\n",
    "print()\n",
    "\n",
    "# Iterate through clustering algorithms and perform bootstrap iterations\n",
    "algo_aucs_boots, algo_optimisms, algo_ls_orig_dfs = {}, {}, {}\n",
    "\n",
    "for algo_name, algo in algorithms.items():\n",
    "    method_aucs_boots, method_optimisms, method_ls_orig_dfs = [\n",
    "        { method: [] for method in [\"eucl_sil\", \"eucl_gap\", \"gow_sil\", \"gow_gap\"] }\n",
    "        for _ in range(3)\n",
    "    ]\n",
    "    optimism_sil_avg, std_auc_boot = {}, {}\n",
    "\n",
    "    # Perform bootstrap iterations\n",
    "    for i in range(n_bootstraps):\n",
    "        sample = resample(orig_data, replace=True, n_samples=4509)\n",
    "        sample_gap = resample(orig_data_gap, replace=True, n_samples=4509)\n",
    "        y_boot = sample['y_orig']\n",
    "\n",
    "        for method in [\"eucl_sil\", \"eucl_gap\", \"gow_sil\", \"gow_gap\"]:\n",
    "            func, args = retrieve_n(method, algo, sample.iloc[:, 1:], sample_gap.iloc[:, 1:], range_n_clusters)\n",
    "            n = func(*args)[0] if algo_name.endswith(\"sil\") else func(*args)\n",
    "            clusterer, l_orig = auc_bootstrap(algo_name, method, n, sample.iloc[:, 1:], orig_data.iloc[:, 1:])\n",
    "            l_orig = clusterer(*l_orig)\n",
    "\n",
    "            # (2) AUC Bootstrap\n",
    "            try:\n",
    "                l_boot = pd.DataFrame(clusterer.labels_)\n",
    "                df_boot = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"l_boot\": np.ravel(l_boot)})\n",
    "                m_ = sm.formula.glm(formula=\"y_boot ~ l_boot\", family=sm.families.Binomial(), data=df_boot).fit()\n",
    "                \n",
    "                # Calculate AUC Boot\n",
    "                y_pred_boot = m_.predict(l_boot)\n",
    "                auc_boot = roc_auc_score(y_boot, y_pred_boot)\n",
    "\n",
    "                # Calculate AUC Original\n",
    "                y_pred_orig = m_.predict(pd.DataFrame(df_orig['labels_orig']))\n",
    "                auc_orig = roc_auc_score(df_orig['y_orig'], y_pred_orig)\n",
    "\n",
    "                # Calculate optimism and append results\n",
    "                optimism = auc_boot - auc_orig\n",
    "\n",
    "                # Append to the respective lists\n",
    "                method_aucs_boots[method].append(auc_boot)\n",
    "                method_optimisms[method].append(optimism)\n",
    "                method_ls_orig_dfs[method].append(l_orig.reset_index(drop=True))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"An error occurred:\", e)\n",
    "                continue\n",
    "\n",
    "    # Aggregate and compile\n",
    "    optimism_avg = {k: np.sum(d) / n_bootstraps for k,d in method_optimisms.items() }\n",
    "    std_auc_boot[method] = {k: np.std(d) for k,d in method_aucs_boot.items() }\n",
    "    method_ls_orig_dfs = {k:pd.concat(dfs, axis=1) for k,dfs in method_ls_orig_dfs.items()}\n",
    "\n",
    "    # Assign to the algo-level dicts\n",
    "    algo_aucs_boots[algo] = method_aucs_boot\n",
    "    algo_optimisms[algo] = method_optimisms\n",
    "    algo_ls_orig_dfs[algo] = method_ls_orig_dfs\n",
    "    \n",
    "#     print(\n",
    "#         f\"Algorithm: {algo_name}\\n\"\n",
    "#         f\"Optimism Metrics:\\n\"\n",
    "#         f\"  - Euclidean Silhouette: {optimism_avg[\"eucl_sil\"]}\\n\"\n",
    "#         f\"  - Euclidean Gap: {optimism_avg[\"eucl_gap\"]}\\n\"\n",
    "#         f\"  - Gower Silhouette: {optimism_avg[\"gow_sil\"]}\\n\"\n",
    "#         f\"  - Gower Gap: {optimism_avg[\"gow_gap\"]}\\n\"\n",
    "\n",
    "#         f\"Standard Deviation of Bootstrap AUCs:\\n\"\n",
    "#         f\"  - Euclidean Silhouette: {std_auc_boot[\"eucl_sil\"]}\\n\"\n",
    "#         f\"  - Euclidean Gap: {std_auc_boot[\"eucl_gap\"]}\\n\"\n",
    "#         f\"  - Gower Silhouette: {std_auc_boot[\"gow_sil\"]}\\n\"\n",
    "#         f\"  - Gower Gap: {std_auc_boot[\"gow_gap\"]}\\n\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e75f96",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Scenario 1: OLS of adjusted Rand Stability Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a2f24",
   "metadata": {},
   "source": [
    "In this cell we perform ordinary least squares to identify a possible relationship between the the adjusted Rand stability values for Scenario 1 and clustering algorithm, distance measure and/or cluster selection method. We recognize that the amount of observations will limit us in obtaining statistical significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be54b21",
   "metadata": {
    "scrolled": false,
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'algorithm':\n",
    "    ['AC', 'KM', 'SC', 'AC', 'KM', 'SC', 'AC', 'KM', 'SC', 'AC', 'KM', 'SC'],\n",
    "    'distance': [\n",
    "        'Gow', 'Gow', 'Gow', 'Eucl', 'Eucl', 'Eucl', 'Gow', 'Gow', 'Gow',\n",
    "        'Eucl', 'Eucl', 'Eucl'\n",
    "    ],\n",
    "    'method': [\n",
    "        'Sil', 'Sil', 'Sil', 'Sil', 'Sil', 'Sil', 'Gap', 'Gap', 'Gap', 'Gap',\n",
    "        'Gap', 'Gap'\n",
    "    ],\n",
    "    'value':\n",
    "    \n",
    "    [0.47, 0.20, 0.98, 0.60, 0.67, 0.57, 0.96, 0.34, 0.37, 0.61, 0.77, 0.66]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df,\n",
    "                            columns=['algorithm', 'distance', 'method'],\n",
    "                            prefix=['algorithm', 'distance', 'method'],\n",
    "                            drop_first=True)\n",
    "\n",
    "X = df_encoded[['algorithm_KM', 'algorithm_SC', 'distance_Gow', 'method_Sil']]\n",
    "y = df['value']\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS (Ordinary Least Squares) model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary of the logistic regression model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22d96e",
   "metadata": {},
   "source": [
    "## Scenario 1: SHAP Analysis <a class=\"anchor\" id=\"chapter10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a2f23",
   "metadata": {},
   "source": [
    "In this cell we perform SHAP analysis to identify feature importances in the cluster design of each method. Larger SHAP values imply more importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y1 = cluster_labels['AC-Gow-Sil']\n",
    "y2 = cluster_labels['KM-Gow-Sil']\n",
    "y3 = cluster_labels['SC-Gow-Sil']\n",
    "y4 = cluster_labels['AC-Eucl-Sil']\n",
    "y5 = cluster_labels['KM-Eucl-Sil']\n",
    "y6 = cluster_labels['SC-Eucl-Sil']\n",
    "y7 = cluster_labels['AC-Gow-Gap']\n",
    "y8 = cluster_labels['KM-Gow-Gap']\n",
    "y9 = cluster_labels['SC-Gow-Gap']\n",
    "y10 = cluster_labels['AC-Eucl-Gap']\n",
    "y11 = cluster_labels['KM-Eucl-Gap']\n",
    "y12 = cluster_labels['SC-Eucl-Gap']\n",
    "\n",
    "cluster_labels_list = [(y1, 'AC-Gow-Sil'), (y2, 'KM-Gow-Sil'),\n",
    "                       (y3, 'SC-Gow-Sil'), (y4, 'AC-Eucl-Sil'),\n",
    "                       (y5, 'KM-Eucl-Sil'), (y6, 'SC-Eucl-Sil'),\n",
    "                       (y7, 'AC-Gow-Gap'), (y8, 'KM-Gow-Gap'),\n",
    "                       (y9, 'SC-Gow-Gap'), (y10, 'AC-Eucl-Gap'),\n",
    "                       (y11, 'KM-Eucl-Gap'), (y12, 'SC-Eucl-Gap')]\n",
    "\n",
    "\n",
    "def shap_feature_ranking(data, shap_values, columns=None):\n",
    "    if columns is None:\n",
    "        columns = data.columns.tolist()\n",
    "\n",
    "    # Get column indices for the specified columns\n",
    "    c_idxs = [data.columns.get_loc(column) for column in columns]\n",
    "\n",
    "    # Calculate mean shap values\n",
    "    if isinstance(shap_values, list):\n",
    "        means = [\n",
    "            np.abs(shap_values[class_][:, c_idxs]).mean(axis=0)\n",
    "            for class_ in range(len(shap_values))\n",
    "        ]\n",
    "        shap_means = np.sum(np.column_stack(means), axis=1)\n",
    "    else:\n",
    "        assert len(shap_values.shape) == 2, 'Expected two-dimensional shap values array.'\n",
    "        shap_means = np.abs(shap_values[:, c_idxs]).mean(axis=0)\n",
    "\n",
    "    df_ranking = pd.DataFrame({\n",
    "        'feature': columns,\n",
    "        'mean_shap_value': shap_means\n",
    "    })\n",
    "\n",
    "    df_ranking = df_ranking.sort_values(by='mean_shap_value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df_ranking.index += 1\n",
    "\n",
    "    return df_ranking\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Perform SHAP analysis for each cluster label\n",
    "for i, (cluster_label, label_name) in enumerate(cluster_labels_list, start=1):\n",
    "    # Create and fit the RandomForestClassifier\n",
    "    clf.fit(cluster_vars_encoded, cluster_label)\n",
    "\n",
    "    # Create a TreeExplainer\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "\n",
    "    # Calculate SHAP values for all samples\n",
    "    shap_values = explainer.shap_values(cluster_vars_encoded)\n",
    "\n",
    "    # Get the feature ranking\n",
    "    feature_ranking = shap_feature_ranking(cluster_vars_encoded, shap_values)\n",
    "\n",
    "    # Align feature order based on the first model's order\n",
    "    if i == 1:\n",
    "        feature_order = feature_ranking['feature'].tolist()\n",
    "    else:\n",
    "        feature_ranking = feature_ranking.set_index('feature').reindex(feature_order).reset_index()\n",
    "\n",
    "    # Print the model name\n",
    "    print(f'Model: {label_name}')\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(feature_ranking)\n",
    "    print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d6ba8",
   "metadata": {},
   "source": [
    "## GridSearch Gaussian Mixture <a class=\"anchor\" id=\"chapter11\"></a>\n",
    " \n",
    "Sklearn's default value for the number of components in the Gaussian Mixture clustering algorithm is 1. Therefore, we performed a grid search to find an alternative number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdd445",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(1, 35),\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV 20 times\n",
    "for i in range(20):\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=GaussianMixture(),\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=gmm_bic_score)\n",
    "\n",
    "    # Fit the model using scaled encoded cluster variables\n",
    "    grid_search.fit(cluster_vars_encoded_scaled)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(grid_search.cv_results_,\n",
    "                      columns=[\n",
    "                          \"param_n_components\", \"param_covariance_type\",\n",
    "                          \"mean_test_score\"\n",
    "                      ])\n",
    "\n",
    "    # Calculate BIC score and update DataFrame\n",
    "    df[\"BIC score\"] = -df[\"mean_test_score\"]\n",
    "    df.drop('mean_test_score', axis=1, inplace=True)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        \"param_n_components\": \"Number of components\",\n",
    "        \"param_covariance_type\": \"Type of covariance\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Sort DataFrame by BIC score\n",
    "    df.sort_values(by=\"BIC score\", inplace=True)\n",
    "\n",
    "    # Display the top rows of the DataFrame\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c653f2b",
   "metadata": {},
   "source": [
    "## Scenario 2 <a class=\"anchor\" id=\"chapter12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94677eeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_algorithms():\n",
    "\n",
    "    kmeans = KMeans()\n",
    "    kmedoids = KMedoids()\n",
    "    gmm = GaussianMixture(n_components=45, covariance_type='diag')\n",
    "    average_linkage = AgglomerativeClustering(linkage='average')\n",
    "    ward = AgglomerativeClustering(linkage='ward')\n",
    "    spectral = SpectralClustering(affinity='nearest_neighbors')\n",
    "    birch = Birch()\n",
    "    mean_shift = MeanShift()\n",
    "    affinity_propagation = AffinityPropagation()\n",
    "    dbscan = DBSCAN()\n",
    "    optics = OPTICS()\n",
    "\n",
    "    return [('KMeans', kmeans), ('KMedoids', kmedoids),\n",
    "            ('GaussianMixture', gmm),\n",
    "            ('AgglomerativeClustering', average_linkage), ('Ward', ward),\n",
    "            ('SpectralClustering', spectral), ('BIRCH', birch),\n",
    "            ('Mean Shift', mean_shift),\n",
    "            ('AffinityPropagation', affinity_propagation), ('DBSCAN', dbscan),\n",
    "            ('OPTICS', optics)]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 4,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"random_state\": 42,\n",
    "    \"metric\": 'precomputed'\n",
    "}\n",
    "\n",
    "clustering_algorithms = prepare_algorithms()\n",
    "\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = {}\n",
    "\n",
    "    for algorithm_name, algorithm in clustering_algorithms:\n",
    "        try:\n",
    "            algorithm.fit(data)\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels[algorithm_name] = algorithm.labels_.astype(int)\n",
    "            elif hasattr(algorithm, 'predict'):\n",
    "                labels[algorithm_name] = algorithm.predict(data)\n",
    "            else:\n",
    "                raise AttributeError(\n",
    "                    f\"{algorithm_name} does not have a labels_ attribute or a predict method.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {algorithm_name} could not be fitted. {e}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "cluster_labels = get_labels(cluster_vars_encoded_scaled)\n",
    "\n",
    "\n",
    "def print_label_counts(cluster_labels, X):\n",
    "    if not cluster_labels:\n",
    "        print(\"Error: Input is empty.\")\n",
    "        return\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        label_counts = Counter(labels)\n",
    "        cluster_sizes = list(label_counts.values())\n",
    "\n",
    "        print(f\"Algorithm: {algorithm_name}\")\n",
    "        print(f\"Number of unique labels: {len(label_counts)}\")\n",
    "        print(f\"Largest: {max(cluster_sizes)}\")\n",
    "        print(f\"Smallest: {min(cluster_sizes)}\")\n",
    "        print(f\"Average: {np.mean(cluster_sizes)}\")\n",
    "        print(f\"Median: {np.median(cluster_sizes)}\")\n",
    "\n",
    "        if len(label_counts) == 1:\n",
    "            print(\"Silhouette score: Not applicable (only one cluster)\\n\")\n",
    "        else:\n",
    "            silhouette = silhouette_score(X, labels, metric='euclidean')\n",
    "            print(f\"Silhouette score: {silhouette}\\n\")\n",
    "\n",
    "\n",
    "print_label_counts(cluster_labels, cluster_vars_encoded_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b75699",
   "metadata": {},
   "source": [
    "## Print the Top Ten Largest Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9118e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store label counts for each method\n",
    "method_label_counts = {}\n",
    "\n",
    "# Iterate through each method in the dictionary\n",
    "for method, labels in cluster_labels.items():\n",
    "    # Count the occurrences of each label for the current method\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    # Store the label counts in the dictionary\n",
    "    method_label_counts[method] = label_counts\n",
    "\n",
    "# Print the top 10 label counts for each method\n",
    "for method, label_counts in method_label_counts.items():\n",
    "    print(f\"{method} Top 10 Label Counts:\")\n",
    "    for label, count in label_counts.most_common(10):\n",
    "        print(f\"Label: {label}, Count: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb5fbd",
   "metadata": {},
   "source": [
    "## Scenario 2: SHAP Analysis <a class=\"anchor\" id=\"chapter13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a872f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def shap_feature_ranking(data, shap_values, columns=[]):\n",
    "    if not columns: columns = data.columns.tolist()\n",
    "\n",
    "    c_idxs = []\n",
    "    for column in columns:\n",
    "        c_idxs.append(data.columns.get_loc(column))\n",
    "    if isinstance(shap_values, list):\n",
    "        means = [\n",
    "            np.abs(shap_values[class_][:, c_idxs]).mean(axis=0)\n",
    "            for class_ in range(len(shap_values))\n",
    "        ]\n",
    "        shap_means = np.sum(np.column_stack(means), 1)\n",
    "    else:\n",
    "        assert len(shap_values.shape\n",
    "                   ) == 2, 'Expected two-dimensional shap values array.'\n",
    "        shap_means = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    df_ranking = pd.DataFrame({\n",
    "        'feature': columns,\n",
    "        'mean_shap_value': shap_means\n",
    "    }).sort_values(by='mean_shap_value',\n",
    "                   ascending=False).reset_index(drop=True)\n",
    "    df_ranking.index += 1\n",
    "\n",
    "    return df_ranking\n",
    "\n",
    "\n",
    "y1 = cluster_labels['AffinityPropagation']\n",
    "y2 = cluster_labels['BIRCH']\n",
    "y3 = cluster_labels['AgglomerativeClustering']\n",
    "y4 = cluster_labels['DBSCAN']\n",
    "y5 = cluster_labels['GaussianMixture']\n",
    "y6 = cluster_labels['Ward']\n",
    "y7 = cluster_labels['KMeans']\n",
    "y8 = cluster_labels['KMedoids']\n",
    "y9 = cluster_labels['Mean Shift']\n",
    "y10 = cluster_labels['SpectralClustering']\n",
    "y11 = cluster_labels['OPTICS']\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "cluster_labels_list = [(y1, 'AffinityPropagation'), (y2, 'BIRCH'),\n",
    "                       (y3, 'AgglomerativeClustering'), (y4, 'DBSCAN'),\n",
    "                       (y5, 'GaussianMixture'), (y6, 'Ward'), (y7, 'KMeans'),\n",
    "                       (y8, 'KMedoids'), (y9, 'MeanShift'),\n",
    "                       (y10, 'SpectralClustering'), (y11, 'OPTICS')]\n",
    "\n",
    "all_models = pd.DataFrame()\n",
    "\n",
    "# Perform SHAP analysis for each cluster label\n",
    "for i, (cluster_label, label_name) in enumerate(cluster_labels_list, start=1):\n",
    "    # Create and fit the RandomForestClassifier\n",
    "    clf.fit(cluster_vars_encoded_scaled, cluster_label)\n",
    "\n",
    "    # Create a TreeExplainer\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "\n",
    "    # Calculate SHAP values for all samples\n",
    "    shap_values = explainer.shap_values(cluster_vars_encoded_scaled)\n",
    "\n",
    "    # Get the feature ranking\n",
    "    feature_ranking = shap_feature_ranking(cluster_vars_encoded_scaled,\n",
    "                                           shap_values)\n",
    "\n",
    "    # Align feature order based on the first model's order\n",
    "    if i == 1:\n",
    "        feature_order = feature_ranking['feature'].tolist()\n",
    "    else:\n",
    "        feature_ranking = feature_ranking.set_index('feature').reindex(\n",
    "            feature_order).reset_index()\n",
    "\n",
    "    # Append the feature ranking to the final dataframe\n",
    "    all_models = pd.concat([all_models, feature_ranking], ignore_index=True)\n",
    "\n",
    "    # Print the model name\n",
    "    print(f'Model: {label_name}')\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(feature_ranking)\n",
    "    print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bab549",
   "metadata": {},
   "source": [
    "## Scenario 2: Prognostic Value & Stability Bootstrap\n",
    "<a class=\"anchor\" id=\"chapter14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27252a8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_data = pd.concat([y_dich, cluster_vars_encoded_scaled], axis=1)\n",
    "orig_data.columns = ['y_orig'] + list(cluster_vars_encoded_scaled.columns)\n",
    "\n",
    "# (1) AUC Apparent\n",
    "aucs_apparent = {}\n",
    "\n",
    "for algorithm_name, labels in cluster_labels.items():\n",
    "    df = pd.DataFrame({\"y_orig\": np.ravel(y_dich), \"labels\": np.ravel(labels)})\n",
    "    formula = \"y_orig ~ labels\"\n",
    "    model = sm.formula.glm(formula=formula, family=sm.families.Binomial(), data=df).fit()\n",
    "    y_pred_apparent = model.predict(df['labels'])\n",
    "    \n",
    "    aucs_apparent[algorithm_name] = auc_apparent\n",
    "    \n",
    "for algorithm_name, algorithm in clustering_algorithms:\n",
    "    \n",
    "    n_bootstraps = 10\n",
    "    aucs_bootstrap = []\n",
    "    optimisms = []\n",
    "    labels_origs = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        sample = resample(orig_data, replace=True, n_samples=4509)\n",
    "\n",
    "        y_boot = sample.iloc[:,0]\n",
    "        \n",
    "        # (2) AUC Bootstrap\n",
    "        try:\n",
    "            algorithm.fit(sample.iloc[:,1:])\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels_boot = pd.DataFrame(algorithm.labels_)\n",
    "            elif hasattr(algorithm, 'predict'):\n",
    "                labels_boot = pd.DataFrame(algorithm.predict(sample.iloc[:,1:]))\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "            continue\n",
    "        \n",
    "        boot_data = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"labels_boot\": np.ravel(labels_boot)})\n",
    "\n",
    "        model = sm.formula.glm(formula=\"y_boot ~ labels_boot\", family=sm.families.Binomial(), data=boot_data).fit()\n",
    "\n",
    "        y_pred_boot = model.predict(labels_boot) # Probabilties between 0 / 1\n",
    "\n",
    "        auc_bootstrap = roc_auc_score(y_boot, y_pred_boot)\n",
    "        \n",
    "        aucs_bootstrap.append(auc_bootstrap)\n",
    "        \n",
    "        # (3) AUC Original \n",
    "        if isinstance(algorithm, (AgglomerativeClustering, SpectralClustering, MeanShift, DBSCAN, OPTICS)):\n",
    "            labels_orig = pd.DataFrame(algorithm.fit_predict(orig_data.iloc[:,1:]))\n",
    "        else:\n",
    "            labels_orig = pd.DataFrame(algorithm.predict(orig_data.iloc[:,1:]))\n",
    "\n",
    "        y_pred_orig = model.predict(labels_orig)\n",
    "\n",
    "        auc_original = roc_auc_score(df['y_orig'], y_pred_orig)\n",
    "        \n",
    "        optimism = auc_bootstrap - auc_original\n",
    "\n",
    "        optimisms.append(optimism)\n",
    "        \n",
    "        #\n",
    "        \n",
    "        labels_origs = pd.concat([labels_origs, labels_orig], axis=1)\n",
    "     \n",
    "    optimism_avg = np.mean(optimisms) # Should be positive\n",
    "    \n",
    "    aucs_o = aucs_apparent[algorithm_name] - optimism_avg\n",
    "    \n",
    "    ci = [aucs_o - 1.96 * np.std(aucs_bootstrap), aucs_o + 1.96 * np.std(aucs_bootstrap)]\n",
    "        \n",
    "    print(f\"Algorithm: {algorithm_name}\\n\"\n",
    "      f\"Optimism Average: {optimism_avg}\\n\"\n",
    "      f\"AUC Apparent: {aucs_apparent[algorithm_name]}\\n\"\n",
    "      f\"AUC Optimism Adjusted: {aucs_o}\\n\"\n",
    "      f\"Confidence Interval: {ci}\")\n",
    "\n",
    "    # Stability\n",
    "    \n",
    "    stability_bootstrap(labels_origs, n_bootstraps)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe837f",
   "metadata": {},
   "source": [
    "##  Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_vectors(c):\n",
    "    v = [[], []]\n",
    "\n",
    "    for i, row in enumerate(c):\n",
    "        for j, val in enumerate(row):\n",
    "            v[0].extend([i] * val)\n",
    "            v[1].extend([j] * val)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc831feb",
   "metadata": {},
   "source": [
    "## Agreement: Pair-Confusion Matrix <a class=\"anchor\" id=\"chapter15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d00fcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def agreement_pairconf(cluster_labels):\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "    names = []\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    cluster_labels = pd.DataFrame.from_dict(cluster_labels)\n",
    "\n",
    "    # Get all pairwise combinations of the columns\n",
    "    pairwise_combinations = pd.DataFrame(\n",
    "        list(combinations(cluster_labels.columns, 2)))\n",
    "\n",
    "    # Iterate over all pairwise combinations\n",
    "    for i in range(len(pairwise_combinations)):\n",
    "\n",
    "        pairconf = pair_confusion_matrix(\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 0]],\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 1]])\n",
    "\n",
    "        a = pairconf[1, 1]  # pairs grouped together in both\n",
    "        b = pairconf[1, 0]\n",
    "        c = pairconf[0, 1]\n",
    "        d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "        rand = (a + d) / (a + d + b + c)\n",
    "        arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) * (d + c))\n",
    "\n",
    "        rands.append(rand)\n",
    "        arands.append(arand)\n",
    "\n",
    "        names.append(\n",
    "            f\"{pairwise_combinations.iloc[i,0]}_{pairwise_combinations.iloc[i,1]}\"\n",
    "        )\n",
    "\n",
    "    scores = pd.DataFrame({\n",
    "        'Rand': rands,\n",
    "        'Adjusted Rand': arands\n",
    "    },\n",
    "                          index=names)\n",
    "\n",
    "    return scores.sort_index(\n",
    "    )  # scores.sort_values('Adjusted Rand', ascending=False)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "agreement_pairconf(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d1e3e",
   "metadata": {},
   "source": [
    "## Agreement: Contingency Table <a class=\"anchor\" id=\"chapter16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212bf0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def agreement_contingency(cluster_labels):\n",
    "    scores = {}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    cluster_labels = pd.DataFrame.from_dict(cluster_labels)\n",
    "\n",
    "    # Get all pairwise combinations of the columns\n",
    "    pairwise_combinations = pd.DataFrame(\n",
    "        list(combinations(cluster_labels.columns, 2)))\n",
    "\n",
    "    # Iterate over all pairwise combinations\n",
    "    for i in range(len(pairwise_combinations)):\n",
    "        contingency = contingency_matrix(\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 0]],\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 1]])\n",
    "\n",
    "        mi = mutual_info_score(_, _, contingency=contingency)\n",
    "        ami = adjusted_mutual_info_score(\n",
    "            produce_vectors(contingency)[0],\n",
    "            produce_vectors(contingency)[1])\n",
    "\n",
    "        scores[pairwise_combinations.iloc[i, 0] + '_' +\n",
    "               pairwise_combinations.iloc[i, 1]] = {\n",
    "                   'Mutual Information': mi,\n",
    "                   'Adjusted Mutual Information': ami\n",
    "               }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    scores = pd.DataFrame.from_dict(scores, orient='index')\n",
    "\n",
    "    return scores.sort_index(\n",
    "    )  # scores.sort_values('Adjusted Mutual Information', ascending=False)\n",
    "\n",
    "\n",
    "agreement_contingency(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd49de8",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Input for the UpSet Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e534b1",
   "metadata": {
    "scrolled": false,
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def binary_matrix(cluster_labels):\n",
    "\n",
    "    binary = []\n",
    "    data_dict = {}\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        n = len(labels)\n",
    "        labels_repeated = labels.repeat(n).reshape(n, n)\n",
    "        labels_transposed = labels_repeated.transpose()\n",
    "        temp = (labels_repeated == labels_transposed).astype(int)\n",
    "        col = temp[np.triu_indices(labels_repeated.shape[0], k=1)].transpose()\n",
    "        binary.append(algorithm_name)\n",
    "        binary.append(col)\n",
    "\n",
    "    for i, item in enumerate(binary):\n",
    "        if i % 2 == 0:\n",
    "            # The item is a column name\n",
    "            col_name = item\n",
    "        else:\n",
    "            data_dict[col_name] = item\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "binary_matrix(cluster_labels)\n",
    "\n",
    "#\n",
    "\n",
    "N = len(next(iter(cluster_labels.values())))\n",
    "K = len(cluster_labels)\n",
    "\n",
    "# Convert labels dictionary to numpy array\n",
    "A = np.zeros((N, K))\n",
    "for i, label_list in enumerate(cluster_labels.values()):\n",
    "    A[:, i] = label_list\n",
    "\n",
    "# Create an array to store the group results for each algorithm\n",
    "group_results = []\n",
    "\n",
    "# Iterate over the algorithms\n",
    "for i, label_list in enumerate(cluster_labels.values()):\n",
    "    # Create an empty array to store the group results for the pairs\n",
    "    algorithm_group = np.zeros(N * (N - 1) // 2, dtype=int)\n",
    "    idx = 0\n",
    "    # Iterate over the data point pairs\n",
    "    for j in range(N):\n",
    "        for k in range(j + 1, N):\n",
    "            # Check if the algorithm groups the pair or not\n",
    "            if label_list[j] == label_list[k]:\n",
    "                algorithm_group[idx] = 1\n",
    "            idx += 1\n",
    "    group_results.append(algorithm_group)\n",
    "\n",
    "# Create a Pandas DataFrame with the group results\n",
    "df = pd.DataFrame(np.column_stack(group_results),\n",
    "                  columns=cluster_labels.keys())\n",
    "\n",
    "#\n",
    "\n",
    "# Convert pandas dataframe to R dataframe\n",
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    r_df = robjects.conversion.py2rpy(df)\n",
    "\n",
    "# Save R dataframe as .rds file\n",
    "r_file = \"M1_BinaryMatrix.rds\"\n",
    "robjects.r.assign(\"my_df_tosave\", r_df)\n",
    "robjects.r(f\"saveRDS(my_df_tosave, file='{r_file}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdf1d2",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## 'Intersection' Bootstrap Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1e74c",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def stability_pairconf(data, algorithm, n_samples, n_iter=200):\n",
    "\n",
    "    labels = []\n",
    "    intersect = []\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for x in range(n_iter):\n",
    "        boot = resample(data, replace=True, n_samples=n_samples)\n",
    "        algorithm = algorithm.fit(boot)\n",
    "        labels.append(boot.index)\n",
    "\n",
    "        if isinstance(algorithm, GaussianMixture):\n",
    "            labels.append(algorithm.predict(boot))\n",
    "        else:\n",
    "            labels.append(algorithm.labels_)\n",
    "\n",
    "    labels = np.array(labels).transpose()\n",
    "    labels = pd.DataFrame(labels)\n",
    "\n",
    "    for col_pos_1 in range(n_iter):\n",
    "        for col_pos_2 in range(col_pos_1 + 1, n_iter):\n",
    "            if col_pos_1 == col_pos_2:\n",
    "                continue\n",
    "\n",
    "            intersect = list(\n",
    "                set(labels[col_pos_1 * 2]) & set(labels[col_pos_2 * 2]))\n",
    "\n",
    "            run_1_pred = labels[[col_pos_1 * 2, col_pos_1 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_1 * 2].isin(intersect)]\n",
    "            run_2_pred = labels[[col_pos_2 * 2, col_pos_2 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_2 * 2].isin(intersect)]\n",
    "\n",
    "            run_1_pred = run_1_pred.sort_values(\n",
    "                by=col_pos_1 * 2).drop_duplicates(subset=[col_pos_1 * 2])\n",
    "            run_2_pred = run_2_pred.sort_values(\n",
    "                by=col_pos_2 * 2).drop_duplicates(subset=[col_pos_2 * 2])\n",
    "\n",
    "            run_1_pred = run_1_pred[col_pos_1 * 2 + 1]\n",
    "            run_2_pred = run_2_pred[col_pos_2 * 2 + 1]\n",
    "\n",
    "            pairconf = pair_confusion_matrix(run_1_pred, run_2_pred)\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) *\n",
    "                                           (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each algorithm in clustering_algorithms\n",
    "for algorithm, algorithm_function in [\n",
    "        clustering_algorithms[3], clustering_algorithms[4],\n",
    "        clustering_algorithms[5], clustering_algorithms[7],\n",
    "        clustering_algorithms[9], clustering_algorithms[10]\n",
    "]:\n",
    "    print(algorithm_function)\n",
    "\n",
    "    # Call the stability_pairconf function with the appropriate parameters\n",
    "    result = stability_pairconf(cluster_vars_encoded_scaled,\n",
    "                                algorithm_function,\n",
    "                                n_samples=4509,\n",
    "                                n_iter=200)\n",
    "\n",
    "    # Assign a name to the row based on the algorithm\n",
    "    result.name = algorithm\n",
    "\n",
    "    # Append the result to the DataFrame\n",
    "    results_df = results_df.append(result)\n",
    "\n",
    "    # Print the intermediate result\n",
    "    print(f\"Algorithm: {algorithm}\")\n",
    "    print(result)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(results_df)\n",
    "\n",
    "stability_pairconf(cluster_vars_encoded_scaled,\n",
    "                   OPTICS(),\n",
    "                   n_samples=4509,\n",
    "                   n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b174194",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## 'Intersection' Bootstrap Gower's Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bc535",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def stability_pairconf(data, algorithm, n_samples, n_iter):\n",
    "\n",
    "    labels = []\n",
    "    intersect = []\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for x in range(n_iter):\n",
    "        boot = resample(data, replace=True, n_samples=n_samples)\n",
    "        labels.append(boot.index)\n",
    "        gowerdist = pd.DataFrame(gower.gower_matrix(boot))\n",
    "\n",
    "        if isinstance(algorithm, sklearn.cluster._spectral.SpectralClustering):\n",
    "            gowersim = 1 - gowerdist\n",
    "            algorithm = algorithm.fit(gowersim)\n",
    "            labels.append(algorithm.labels_)\n",
    "        else:\n",
    "            algorithm = algorithm.fit(gowerdist)\n",
    "            labels.append(algorithm.labels_)\n",
    "\n",
    "    labels = pd.DataFrame(np.array(labels).transpose())\n",
    "    print(labels)\n",
    "\n",
    "    for col_pos_1 in range(n_iter):\n",
    "        for col_pos_2 in range(col_pos_1 + 1, n_iter):\n",
    "            if col_pos_1 == col_pos_2:\n",
    "                continue\n",
    "\n",
    "            intersect = list(\n",
    "                set(labels[col_pos_1 * 2]) & set(labels[col_pos_2 * 2]))\n",
    "\n",
    "            run_1_pred = labels[[col_pos_1 * 2, col_pos_1 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_1 * 2].isin(intersect)]\n",
    "            run_2_pred = labels[[col_pos_2 * 2, col_pos_2 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_2 * 2].isin(intersect)]\n",
    "\n",
    "            run_1_pred = run_1_pred.sort_values(\n",
    "                by=col_pos_1 * 2).drop_duplicates(subset=[col_pos_1 * 2])\n",
    "            run_2_pred = run_2_pred.sort_values(\n",
    "                by=col_pos_2 * 2).drop_duplicates(subset=[col_pos_2 * 2])\n",
    "\n",
    "            run_1_pred = run_1_pred[col_pos_1 * 2 + 1]\n",
    "            run_2_pred = run_2_pred[col_pos_2 * 2 + 1]\n",
    "\n",
    "            pairconf = pair_confusion_matrix(run_1_pred, run_2_pred)\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) *\n",
    "                                           (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for algorithm, algorithm_function in [\n",
    "        clustering_algorithms[6], clustering_algorithms[8],\n",
    "        clustering_algorithms[10]\n",
    "]:\n",
    "    print(algorithm_function)\n",
    "\n",
    "    result = stability_pairconf(cluster_vars_encoded,\n",
    "                                algorithm_function,\n",
    "                                n_samples=4509,\n",
    "                                n_iter=200)\n",
    "\n",
    "    result.name = algorithm\n",
    "\n",
    "    results_df = results_df.append(result)\n",
    "\n",
    "    print(f\"Algorithm: {algorithm}\")\n",
    "    print(result)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
