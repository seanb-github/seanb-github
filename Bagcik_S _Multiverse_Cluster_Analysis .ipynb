{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c19031",
   "metadata": {},
   "source": [
    "# Author: S.R.E.A Bagcik\n",
    "### Date: 25-01-2024\n",
    "### E-mail: s.r.e.a.bagcik@lumc.nl / seanbagcik@hotmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0092f",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "* [Packages](#chapter1)\n",
    "* [Data wrangling](#chapter2)\n",
    "* [Silhouette analysis with Euclidean distance](#chapter3)\n",
    "* [Silhouette analysis with Gower's distance](#chapter4)\n",
    "* [Gap method with Euclidean distance](#chapter5)\n",
    "    * [Plotting the Gap statistic](#section_5_1)\n",
    "* [Gap method with Gower's distance](#chapter6)    \n",
    "* [Stability function](#chapter7)\n",
    "* [Scenario 1](#chapter8)\n",
    "* [Prognostic value of clusterings and stability bootstrap in scenario 1](#chapter9)\n",
    "* [SHAP analysis for scenario 1](#chapter10)\n",
    "* [Grid search Gaussian Mixture](#chapter11)\n",
    "* [Scenario 2](#chapter12)\n",
    "* [Prognostic value of clusterings and stability bootstrap in scenario 2](#chapter13)\n",
    "* [SHAP analysis for scenario 2](#chapter14) \n",
    "* [Agreement via the pair-confusion matrix](#chapter15)\n",
    "* [Agreement via the contingency table](#chapter16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d4c7f",
   "metadata": {},
   "source": [
    "## Packages <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c1e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from typing import Callable, Union\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sn\n",
    "import shap\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "import gower\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.stats import sem\n",
    "from sklearn import metrics, mixture\n",
    "from sklearn.cluster import (\n",
    "    AgglomerativeClustering,\n",
    "    AffinityPropagation,\n",
    "    Birch,\n",
    "    DBSCAN,\n",
    "    KMeans,\n",
    "    MeanShift,\n",
    "    OPTICS,\n",
    "    SpectralClustering\n",
    ")\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    adjusted_mutual_info_score,\n",
    "    log_loss,\n",
    "    mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    "    pairwise_distances,\n",
    "    roc_auc_score,\n",
    "    silhouette_samples,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.metrics.cluster import contingency_matrix, pair_confusion_matrix\n",
    "from sklearn.metrics.cluster._expected_mutual_info_fast import expected_mutual_information\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestCentroid, kneighbors_graph\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler\n",
    "from sklearn.utils import check_random_state, resample\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "593b6fff",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Bagcik_S_Multiverse_Cluster_Analysis.ipynb to html\n",
      "[NbConvertApp] Writing 781625 bytes to Bagcik_S_Multiverse_Cluster_Analysis.html\n"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = '/Users/seanvanbork/Desktop/multiverse_cluster_analysis'\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "!jupyter nbconvert --to html --TagRemovePreprocessor.remove_cell_tags={'no'} Bagcik_S_Multiverse_Cluster_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65e499",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12d29f1",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = '/Users/seanvanbork/Desktop/multiverse_cluster_analysis/Data'\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# Read in data\n",
    "cluster_vars = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_cluster_vars.csv'))\n",
    "\n",
    "gower_matrix = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_gower_matrix.csv'))\n",
    "\n",
    "y = pd.read_csv(os.path.join(DATA_DIR, 'OPAL/OPAL_GOSE6monthEndpointDerived.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ea746",
   "metadata": {},
   "source": [
    "## Data wrangling <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "In this code block, we dichotomize the 6-month extended Glasgow Outcome Scale (GOSE). Subsequently, we encode the ordinal categorical feature pupil score. We create dummy variables for the feature injury cause, which is non-ordinal categorical. Last, we scale our features to avoid the scale influencing the Euclidean distance in scenario 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51af3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used in Cluster Analysis\n",
      "\n",
      "1. Subject.Age\n",
      "2. InjuryHx.GCSMotorBaselineDerived\n",
      "3. InjuryHx.GCSScoreBaselineDerived\n",
      "4. InjuryHx.PupilsBaselineDerived\n",
      "5. InjuryHx.EDComplEventHypoxia\n",
      "6. InjuryHx.EDComplEventHypotension\n",
      "7. InjuryHx.InjCause\n",
      "8. InjuryHx.MajorExtracranialInjury\n",
      "9. dim1\n",
      "10. dim2\n",
      "11. dim3\n",
      "12. dim4\n"
     ]
    }
   ],
   "source": [
    "# Dichotomize\n",
    "y_dich = (y >= 5).astype(int)\n",
    "\n",
    "# Encode \"PupilsBaselineDerived\" (Ordinal categorical)\n",
    "encoder = LabelEncoder()\n",
    "cluster_vars_encoded = cluster_vars.copy()\n",
    "cluster_vars_encoded['InjuryHx.PupilsBaselineDerived'] = encoder.fit_transform(\n",
    "    cluster_vars_encoded['InjuryHx.PupilsBaselineDerived'])\n",
    "\n",
    "# Features for reference distribution in Gap Method\n",
    "cluster_vars_gap = cluster_vars_encoded.copy()\n",
    "cluster_vars_gap['InjuryHx.InjCause'] = encoder.fit_transform(\n",
    "    cluster_vars_encoded['InjuryHx.InjCause'])\n",
    "\n",
    "# One-hot-encoding \"InjuryHx.InjCause\" (Categorical, but equal weights)\n",
    "dummy_columns = pd.get_dummies(cluster_vars_encoded['InjuryHx.InjCause'])\n",
    "cluster_vars_encoded = pd.concat(\n",
    "    [cluster_vars_encoded.drop('InjuryHx.InjCause', axis=1), dummy_columns],\n",
    "    axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "columns_to_scale = [\n",
    "    'Subject.Age', 'InjuryHx.GCSMotorBaselineDerived',\n",
    "    'InjuryHx.GCSScoreBaselineDerived', 'dim1', 'dim2', 'dim3', 'dim4'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_vars_encoded[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "non_scaled_columns = [\n",
    "    col for col in cluster_vars_encoded.columns if col not in columns_to_scale\n",
    "]\n",
    "\n",
    "cluster_vars_encoded_scaled = pd.concat(\n",
    "    [cluster_vars_encoded[non_scaled_columns], scaled_df], axis=1)\n",
    "\n",
    "print(\"Features Used in Cluster Analysis\", end='\\n')\n",
    "for i, column in enumerate(cluster_vars.columns, 1):\n",
    "    print(f\"{i}. {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924a4da",
   "metadata": {},
   "source": [
    "## Silhouette analysis with Euclidean distance <a class=\"anchor\" id=\"chapter3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23553bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_silhouette_euclidean(algorithm, data, range_n_clusters):\n",
    "\n",
    "    max_silhouette_avg = -1\n",
    "    best_n_clusters = -1\n",
    "    silhouette_values = [\n",
    "    ]  # List to store silhouette scores for each iteration\n",
    "\n",
    "    # Iterate over range of cluster numbers\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Choose clustering algorithm\n",
    "        if algorithm == KMedoids:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  method='alternate').fit(data)\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  linkage='average').fit(data)\n",
    "        elif algorithm == SpectralClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='nearest_neighbors').fit(data)\n",
    "\n",
    "        cluster_labels = clusterer.labels_\n",
    "\n",
    "        # Calculate silhouette score only if there is more than one cluster\n",
    "        if len(set(cluster_labels)) == 1:\n",
    "            silhouette_avg = 0\n",
    "        else:\n",
    "            silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "\n",
    "        # Append the silhouette score\n",
    "        silhouette_values.append(silhouette_avg)\n",
    "\n",
    "        # Update the best silhouette score\n",
    "        if silhouette_avg > max_silhouette_avg:\n",
    "            max_silhouette_avg = silhouette_avg\n",
    "            best_n_clusters = n_clusters\n",
    "\n",
    "    return best_n_clusters, silhouette_values\n",
    "\n",
    "\n",
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "    best_n_clusters, silhouette_values = best_silhouette_euclidean(\n",
    "        algorithm, cluster_vars_encoded, range(2, 30))\n",
    "\n",
    "    print(\n",
    "        f\"The maximum silhouette score for {algorithm_name} is {max(silhouette_values)} for n_clusters = {best_n_clusters}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2633a8",
   "metadata": {},
   "source": [
    "## Silhouette analysis with Gower's distance <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65201c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_silhouette_gower(algorithm, distance, range_n_clusters):\n",
    "    max_silhouette_avg = -1\n",
    "    best_n_clusters = -1\n",
    "    silhouette_values = [\n",
    "    ]  # List to store silhouette scores for each iteration\n",
    "\n",
    "    # Iterate over range of cluster numbers\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Choose clustering algorithm\n",
    "        if algorithm == KMedoids:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  method='alternate',\n",
    "                                  metric='precomputed').fit(distance)\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='precomputed',\n",
    "                                  linkage='average').fit(distance)\n",
    "        elif algorithm == SpectralClustering:\n",
    "            clusterer = algorithm(n_clusters=n_clusters,\n",
    "                                  affinity='precomputed').fit(1 - distance)\n",
    "\n",
    "        cluster_labels = clusterer.labels_\n",
    "\n",
    "        # Calculate silhouette score only if there is more than one cluster\n",
    "        if len(set(cluster_labels)) == 1:\n",
    "            silhouette_avg = 0\n",
    "        else:\n",
    "            silhouette_avg = silhouette_score(distance,\n",
    "                                              cluster_labels,\n",
    "                                              metric='precomputed')\n",
    "\n",
    "        # Append silhouette score\n",
    "        silhouette_values.append(silhouette_avg)\n",
    "\n",
    "        # Update the best silhouette score\n",
    "        if silhouette_avg > max_silhouette_avg:\n",
    "            max_silhouette_avg = silhouette_avg\n",
    "            best_n_clusters = n_clusters\n",
    "\n",
    "    return best_n_clusters, silhouette_values\n",
    "\n",
    "\n",
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "    best_n_clusters, silhouette_values = best_silhouette_gower(\n",
    "        algorithm, gower_matrix, range(2, 30))\n",
    "\n",
    "    print(\n",
    "        f\"The maximum silhouette score for {algorithm_name} is {max(silhouette_values)} for n_clusters = {best_n_clusters}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f117e",
   "metadata": {},
   "source": [
    "## Gap method with Euclidean distance <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07f5b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapStatistics:\n",
    "\n",
    "    def __init__(self, return_params: bool = True):\n",
    "        self.return_params = return_params\n",
    "\n",
    "    def _calculate_Wks(self, algorithm, K: int, X: pd.DataFrame) -> list:\n",
    "        dummy_columns = pd.get_dummies(X['InjuryHx.InjCause'])\n",
    "        X = pd.concat([X.drop('InjuryHx.InjCause', axis=1), dummy_columns],\n",
    "                      axis=1)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        Wks = []\n",
    "\n",
    "        if algorithm == KMedoids:\n",
    "            for k in np.arange(2, K + 1):\n",
    "                clusterer = algorithm(n_clusters=k, method='alternate').fit(X)\n",
    "                labels = clusterer.predict(X)\n",
    "                centroids = clusterer.cluster_centers_\n",
    "\n",
    "        elif algorithm == AgglomerativeClustering:\n",
    "            for k in np.arange(2, K + 1):\n",
    "                labels = algorithm(n_clusters=k,\n",
    "                                   linkage='average').fit_predict(X)\n",
    "                tmp = NearestCentroid()\n",
    "                tmp.fit(X, labels)\n",
    "                centroids = tmp.centroids_\n",
    "\n",
    "        elif algorithm == SpectralClustering:\n",
    "            for k in np.arange(2, K + 1):\n",
    "                labels = algorithm(n_clusters=k,\n",
    "                                   affinity='nearest_neighbors').fit_predict(X)\n",
    "                tmp = NearestCentroid()\n",
    "                tmp.fit(X, labels)\n",
    "                centroids = tmp.centroids_\n",
    "\n",
    "        Ds = []\n",
    "\n",
    "        for i in range(k):\n",
    "            cluster_array = np.array(X[labels == i])\n",
    "            # FORMULA (1)\n",
    "            if len(np.unique(cluster_array)) > 1:\n",
    "                d = pairwise_distances(cluster_array,\n",
    "                                       centroids[i].reshape(1, -1),\n",
    "                                       metric='euclidean')\n",
    "                Ds.append(np.sum(d))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        pooled = 1 / (2 * len(X))\n",
    "        # FORMULA (2)\n",
    "        Wk = np.sum([D * pooled for D in Ds])\n",
    "        Wks.append(Wk)\n",
    "\n",
    "        return Wks\n",
    "\n",
    "    def _simulate_Wks(self, algorithm, X: pd.DataFrame, K: int,\n",
    "                      n_iterations: int) -> [list, list]:\n",
    "        cat_columns = [3, 4, 5, 6, 7]\n",
    "        cont_columns = [0, 1, 2, 8, 9, 10, 11]\n",
    "\n",
    "        cat = X.iloc[:, cat_columns]\n",
    "        cont = X.iloc[:, cont_columns]\n",
    "\n",
    "        sampled_X = X.copy()\n",
    "\n",
    "        for col in cat.columns:\n",
    "            sampled_X[col] = np.random.randint(low=cat[col].min(),\n",
    "                                               high=cat[col].max(),\n",
    "                                               size=len(X))\n",
    "\n",
    "        for col in cont.columns:\n",
    "            sampled_X[col] = np.random.uniform(low=cont[col].min(),\n",
    "                                               high=cont[col].max(),\n",
    "                                               size=len(X))\n",
    "\n",
    "        simulated_Wks = []\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "\n",
    "            Wks_star = self._calculate_Wks(algorithm=algorithm,\n",
    "                                           K=K,\n",
    "                                           X=sampled_X)\n",
    "            simulated_Wks.append(Wks_star)\n",
    "\n",
    "        sim_Wks = np.array(simulated_Wks)\n",
    "        return sim_Wks\n",
    "\n",
    "    def fit_predict(self,\n",
    "                    algorithm,\n",
    "                    K: int,\n",
    "                    X: pd.DataFrame,\n",
    "                    n_iterations: int = 5):\n",
    "        Wks = self._calculate_Wks(algorithm=algorithm, K=K, X=X)\n",
    "        sim_Wks = self._simulate_Wks(algorithm=algorithm,\n",
    "                                     K=K,\n",
    "                                     X=X,\n",
    "                                     n_iterations=n_iterations)\n",
    "\n",
    "        log_Wks = np.log(Wks)\n",
    "        log_Wks_star = np.log(sim_Wks)\n",
    "\n",
    "        sd_k = np.std(log_Wks_star, axis=0)\n",
    "        sim_sks = np.sqrt(1 + (1 / n_iterations)) * sd_k\n",
    "\n",
    "        gaps = np.mean(log_Wks_star - log_Wks, axis=0)\n",
    "\n",
    "        optimum = 1\n",
    "        max_gap = gaps[0]\n",
    "\n",
    "        # GAP - FORMULA (3)\n",
    "        for i in range(0, len(gaps) - 1):\n",
    "            if gaps[i] >= gaps[i + 1] - sim_sks[i + 1]:\n",
    "                if gaps[i] > max_gap:\n",
    "                    optimum = i\n",
    "                    max_gap = gaps[i]\n",
    "\n",
    "        self.params = {\n",
    "            'Wks': Wks,\n",
    "            'sim_Wks': sim_Wks,\n",
    "            'sim_sks': sim_sks,\n",
    "            'gaps': gaps\n",
    "        }\n",
    "\n",
    "        if self.return_params:\n",
    "            return optimum, self.params\n",
    "        else:\n",
    "            return optimum\n",
    "\n",
    "\n",
    "GapStatEucl = GapStatistics(return_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c13009",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Plotting the Gap statistic <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfb138",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "    optimum, params = GapStatEucl.fit_predict(algorithm,\n",
    "                                              K=15,\n",
    "                                              X=cluster_vars_gap)\n",
    "\n",
    "Wks = GapStatEucl.params['Wks']\n",
    "sim_Wks = GapStatEucl.params['sim_Wks']\n",
    "sim_sks = GapStatEucl.params['sim_sks']\n",
    "gaps = GapStatEucl.params['gaps']\n",
    "\n",
    "log_Wks = np.log(Wks)\n",
    "log_sim_Wks = np.log(np.mean(sim_Wks, axis=0))\n",
    "Wks = Wks - np.max(Wks)\n",
    "\n",
    "\n",
    "def clip_values(arr):\n",
    "    \"\"\"\n",
    "    Helper function to clip values in the array between 0 and 1.\n",
    "    \"\"\"\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = max(0, min(1, arr[i]))\n",
    "    return arr\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Bottom right\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(Wks, '-o', label=\"Wks from Data\")\n",
    "plt.plot(log_Wks, '-o', label=\"Logged Wks from Data\")\n",
    "plt.plot(log_sim_Wks, '-o', color=\"green\", label=f\"Logged Simulated Wks\")\n",
    "plt.title(\"Decrease of Within Cluster Distance\")\n",
    "plt.legend()\n",
    "\n",
    "# Bottom left\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(gaps, '-o', color='r')\n",
    "\n",
    "# Normalize yminx and ymaxsx to be between 0 and 1\n",
    "yminx = ((gaps - sim_sks) - np.min(gaps)) / (np.max(gaps) - np.min(gaps))\n",
    "ymaxsx = ((gaps + sim_sks) - np.min(gaps)) / (np.max(gaps) - np.min(gaps))\n",
    "\n",
    "# Clip values to ensure they are between 0 and 1\n",
    "clipped_yminx = clip_values(yminx)\n",
    "clipped_ymaxsx = clip_values(ymaxsx)\n",
    "\n",
    "# Plot vertical lines with clipped ymin and ymax\n",
    "for i in range(len(gaps)):\n",
    "    plt.axvline(x=i,\n",
    "                ymin=clipped_yminx[i],\n",
    "                ymax=clipped_ymaxsx[i],\n",
    "                color='black')\n",
    "\n",
    "plt.axvline(x=optimum, color='green')\n",
    "plt.title(\"Gap Statistics with optimum K at {}\".format(optimum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524895c",
   "metadata": {},
   "source": [
    "## Gap method with Gower's distance <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c15d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapStatisticsGower:\n",
    "\n",
    "    def __init__(self, return_params: bool = False):\n",
    "\n",
    "        self.return_params = return_params\n",
    "\n",
    "    def _calculate_Wks(self, algorithm, K: int, X: pd.DataFrame) -> list:\n",
    "        dummy_columns = pd.get_dummies(X['InjuryHx.InjCause'])\n",
    "        X = pd.concat([X.drop('InjuryHx.InjCause', axis=1), dummy_columns],\n",
    "                      axis=1)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        Wks = []\n",
    "\n",
    "        for k in np.arange(2, K + 1):\n",
    "            if algorithm == KMedoids:\n",
    "                labels = algorithm(n_clusters=k, method='pam').fit_predict(X)\n",
    "            elif algorithm == AgglomerativeClustering:\n",
    "                labels = AgglomerativeClustering(\n",
    "                    n_clusters=k, linkage='average').fit_predict(X)\n",
    "            elif algorithm == SpectralClustering:\n",
    "                labels = SpectralClustering(\n",
    "                    n_clusters=k, affinity='nearest_neighbors').fit_predict(X)\n",
    "\n",
    "            X['labels'] = labels\n",
    "\n",
    "            Ds = []\n",
    "\n",
    "            for label in np.unique(X['labels']):\n",
    "                data = X[X['labels'] == label].drop(columns=['labels'])\n",
    "                n_k = len(data)\n",
    "                pooled = 1 / (2 * n_k)\n",
    "                d = np.sum(gower.gower_matrix(data))\n",
    "\n",
    "                Ds.append(d)\n",
    "\n",
    "            Wk = np.sum([D * pooled for D in Ds])\n",
    "            Wks.append(Wk)\n",
    "\n",
    "            X.drop(columns=['labels'], inplace=True)\n",
    "\n",
    "        return Wks\n",
    "\n",
    "    def _simulate_Wks(self, algorithm, X: pd.DataFrame, K: int,\n",
    "                      n_iterations: int) -> [list, list]:\n",
    "        cat_columns = [3, 4, 5, 6, 7]\n",
    "        cont_columns = [0, 1, 2, 8, 9, 10, 11]\n",
    "\n",
    "        cat = X.iloc[:, cat_columns]\n",
    "        cont = X.iloc[:, cont_columns]\n",
    "\n",
    "        sampled_X = X.copy()\n",
    "\n",
    "        for col in cat.columns:\n",
    "            sampled_X[col] = np.random.randint(low=cat[col].min(),\n",
    "                                               high=cat[col].max(),\n",
    "                                               size=len(X))\n",
    "\n",
    "        for col in cont.columns:\n",
    "            sampled_X[col] = np.random.uniform(low=cont[col].min(),\n",
    "                                               high=cont[col].max(),\n",
    "                                               size=len(X))\n",
    "\n",
    "        simulated_Wks = []\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "\n",
    "            Wks_star = self._calculate_Wks(algorithm=algorithm,\n",
    "                                           K=K,\n",
    "                                           X=sampled_X)\n",
    "            simulated_Wks.append(Wks_star)\n",
    "\n",
    "        sim_Wks = np.array(simulated_Wks)\n",
    "        return sim_Wks\n",
    "\n",
    "    def fit_predict(self,\n",
    "                    algorithm,\n",
    "                    K: int,\n",
    "                    X: pd.DataFrame,\n",
    "                    n_iterations: int = 5):\n",
    "        Wks = self._calculate_Wks(algorithm=algorithm, K=K, X=X)\n",
    "        sim_Wks = self._simulate_Wks(algorithm=algorithm,\n",
    "                                     K=K,\n",
    "                                     X=X,\n",
    "                                     n_iterations=n_iterations)\n",
    "\n",
    "        log_Wks = np.log(Wks)\n",
    "        log_Wks_star = np.log(sim_Wks)\n",
    "\n",
    "        sd_k = np.std(log_Wks_star, axis=0)\n",
    "        sim_sks = np.sqrt(1 + (1 / n_iterations)) * sd_k\n",
    "\n",
    "        gaps = np.mean(log_Wks_star - log_Wks, axis=0)\n",
    "\n",
    "        optimum = 1\n",
    "        max_gap = gaps[0]\n",
    "\n",
    "        # GAP - FORMULA (3)\n",
    "        for i in range(0, len(gaps) - 1):\n",
    "            if gaps[i] >= gaps[i + 1] - sim_sks[i + 1]:\n",
    "                if gaps[i] > max_gap:\n",
    "                    optimum = i\n",
    "                    max_gap = gaps[i]\n",
    "\n",
    "        optimum = min(max(optimum, 1), K)\n",
    "\n",
    "        if self.return_params == True:\n",
    "            params = {\n",
    "                'Wks': Wks,\n",
    "                'sim_Wks': sim_Wks,\n",
    "                'sim_sks': sim_sks,\n",
    "                'gaps': gaps\n",
    "            }\n",
    "            return optimum, params\n",
    "        else:\n",
    "            return optimum\n",
    "\n",
    "\n",
    "GapStatGow = GapStatisticsGower(return_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac93ce1",
   "metadata": {},
   "source": [
    "## Stability function <a class=\"anchor\" id=\"chapter7\"></a>\n",
    "\n",
    "This function calculates the stability of a clustering method given a number of bootstraps $B=200$ quantified via the Rand and adjusted Rand index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e20094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_bootstrap(labels, n_bootstraps):\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for i in range(0, n_bootstraps - 1):\n",
    "        for j in range(i + 1, n_bootstraps):\n",
    "\n",
    "            pairconf = pair_confusion_matrix(labels.iloc[:, i], labels.iloc[:,j])\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) * (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67737a5",
   "metadata": {},
   "source": [
    "## Scenario 1 <a class=\"anchor\" id=\"chapter8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dfaa9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: KM-Gow-Sil\n",
      "Number of unique labels: 12\n",
      "Smallest: 158\n",
      "Largest: 1167\n",
      "Average: 375.75\n",
      "Median: 248.0\n",
      "Silhouette score: 0.3253745399320117\n",
      "\n",
      "Algorithm: KM-Eucl-Sil\n",
      "Number of unique labels: 2\n",
      "Smallest: 1986\n",
      "Largest: 2523\n",
      "Average: 2254.5\n",
      "Median: 2254.5\n",
      "Silhouette score: 0.5575120008521547\n",
      "\n",
      "Algorithm: AC-Gow-Sil\n",
      "Number of unique labels: 2\n",
      "Smallest: 2\n",
      "Largest: 4507\n",
      "Average: 2254.5\n",
      "Median: 2254.5\n",
      "Silhouette score: 0.4498339110069868\n",
      "\n",
      "Algorithm: AC-Eucl-Sil\n",
      "Number of unique labels: 2\n",
      "Smallest: 1277\n",
      "Largest: 3232\n",
      "Average: 2254.5\n",
      "Median: 2254.5\n",
      "Silhouette score: 0.49600126851626797\n",
      "\n",
      "Algorithm: SC-Gow-Sil\n",
      "Number of unique labels: 2\n",
      "Smallest: 1239\n",
      "Largest: 3270\n",
      "Average: 2254.5\n",
      "Median: 2254.5\n",
      "Silhouette score: 0.39871565441600654\n",
      "\n",
      "Algorithm: SC-Eucl-Sil\n",
      "Number of unique labels: 2\n",
      "Smallest: 1901\n",
      "Largest: 2608\n",
      "Average: 2254.5\n",
      "Median: 2254.5\n",
      "Silhouette score: 0.5572220758415105\n",
      "\n",
      "Algorithm: KM-Gow-Gap\n",
      "Number of unique labels: 15\n",
      "Smallest: 147\n",
      "Largest: 815\n",
      "Average: 300.6\n",
      "Median: 222.0\n",
      "Silhouette score: 0.2707969896463518\n",
      "\n",
      "Algorithm: KM-Eucl-Gap\n",
      "Number of unique labels: 14\n",
      "Smallest: 124\n",
      "Largest: 453\n",
      "Average: 322.07142857142856\n",
      "Median: 327.5\n",
      "Silhouette score: 0.3248952203659035\n",
      "\n",
      "Algorithm: AC-Gow-Gap\n",
      "Number of unique labels: 6\n",
      "Smallest: 1\n",
      "Largest: 4330\n",
      "Average: 751.5\n",
      "Median: 2.5\n",
      "Silhouette score: 0.4206146410865413\n",
      "\n",
      "Algorithm: AC-Eucl-Gap\n",
      "Number of unique labels: 25\n",
      "Smallest: 1\n",
      "Largest: 774\n",
      "Average: 180.36\n",
      "Median: 90.0\n",
      "Silhouette score: 0.30736163388651816\n",
      "\n",
      "Algorithm: SC-Gow-Gap\n",
      "Number of unique labels: 7\n",
      "Smallest: 141\n",
      "Largest: 1069\n",
      "Average: 644.1428571428571\n",
      "Median: 638.0\n",
      "Silhouette score: 0.18794261576795027\n",
      "\n",
      "Algorithm: SC-Eucl-Gap\n",
      "Number of unique labels: 17\n",
      "Smallest: 29\n",
      "Largest: 475\n",
      "Average: 265.2352941176471\n",
      "Median: 316.0\n",
      "Silhouette score: 0.15377874154621887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_algorithms():\n",
    "\n",
    "    kmedoids_sil_G12 = KMedoids(n_clusters=12,\n",
    "                                metric='precomputed',\n",
    "                                method='pam')\n",
    "\n",
    "    kmedoids_sil_E2 = KMedoids(n_clusters=2, method='pam')\n",
    "\n",
    "    agglomerative_sil_G2 = AgglomerativeClustering(n_clusters=2,\n",
    "                                                   affinity='precomputed',\n",
    "                                                   linkage='average')\n",
    "\n",
    "    agglomerative_sil_E2 = AgglomerativeClustering(n_clusters=2,\n",
    "                                                   linkage='average')\n",
    "\n",
    "    spectral_sil_G2 = SpectralClustering(n_clusters=2, affinity='precomputed')\n",
    "\n",
    "    spectral_sil_E2 = SpectralClustering(n_clusters=2,\n",
    "                                         affinity='nearest_neighbors')\n",
    "\n",
    "    kmedoids_gap_G15 = KMedoids(n_clusters=15,\n",
    "                                metric='precomputed',\n",
    "                                method='pam')\n",
    "\n",
    "    kmedoids_gap_E14 = KMedoids(n_clusters=14, method='pam')\n",
    "\n",
    "    agglomerative_gap_G6 = AgglomerativeClustering(n_clusters=6,\n",
    "                                                   affinity='precomputed',\n",
    "                                                   linkage='average')\n",
    "\n",
    "    agglomerative_gap_E25 = AgglomerativeClustering(n_clusters=25,\n",
    "                                                    linkage='average')\n",
    "\n",
    "    spectral_gap_G7 = SpectralClustering(n_clusters=7, affinity='precomputed')\n",
    "\n",
    "    spectral_gap_E17 = SpectralClustering(n_clusters=17,\n",
    "                                          affinity='nearest_neighbors')\n",
    "\n",
    "    return [('KM-Gow-Sil', kmedoids_sil_G12), ('KM-Eucl-Sil', kmedoids_sil_E2),\n",
    "            ('AC-Gow-Sil', agglomerative_sil_G2),\n",
    "            ('AC-Eucl-Sil', agglomerative_sil_E2),\n",
    "            ('SC-Gow-Sil', spectral_sil_G2), ('SC-Eucl-Sil', spectral_sil_E2),\n",
    "            ('KM-Gow-Gap', kmedoids_gap_G15),\n",
    "            ('KM-Eucl-Gap', kmedoids_gap_E14),\n",
    "            ('AC-Gow-Gap', agglomerative_gap_G6),\n",
    "            ('AC-Eucl-Gap', agglomerative_gap_E25),\n",
    "            ('SC-Gow-Gap', spectral_gap_G7), ('SC-Eucl-Gap', spectral_gap_E17)]\n",
    "\n",
    "\n",
    "clustering_algorithms = prepare_algorithms()\n",
    "\n",
    "\n",
    "def get_labels(X, X_distance, X_similarity):\n",
    "    labels = {}\n",
    "\n",
    "    for algorithm_name, algorithm in clustering_algorithms:\n",
    "        try:\n",
    "            if algorithm_name.startswith('SC') and algorithm_name.endswith(\n",
    "                ('Gow-Sil', 'Gow-Gap')):\n",
    "                algorithm.fit(X_similarity)\n",
    "            elif algorithm_name.startswith(\n",
    "                ('KM', 'AC')) and algorithm_name.endswith(\n",
    "                    ('Gow-Sil', 'Gow-Gap')):\n",
    "                algorithm.fit(X_distance)\n",
    "            else:\n",
    "                algorithm.fit(X)\n",
    "\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels[algorithm_name] = algorithm.labels_.astype(int)\n",
    "            else:\n",
    "                raise AttributeError(\n",
    "                    f\"{algorithm_name} does not have a labels_ attribute.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {algorithm_name} could not be fitted. {e}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "cluster_labels = get_labels(cluster_vars_encoded, gower_matrix,\n",
    "                            1 - gower_matrix)\n",
    "\n",
    "\n",
    "def print_label_counts(cluster_labels, X, X_distance):\n",
    "    if not cluster_labels:\n",
    "        print(\"Error: Input is empty.\")\n",
    "        return\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        label_counts = Counter(labels)\n",
    "        cluster_sizes = list(label_counts.values())\n",
    "\n",
    "        print(f\"Algorithm: {algorithm_name}\")\n",
    "        print(f\"Number of unique labels: {len(label_counts)}\")\n",
    "        print(f\"Smallest: {min(cluster_sizes)}\")\n",
    "        print(f\"Largest: {max(cluster_sizes)}\")\n",
    "        print(f\"Average: {np.mean(cluster_sizes)}\")\n",
    "        print(f\"Median: {np.median(cluster_sizes)}\")\n",
    "\n",
    "        if len(label_counts) == 1:\n",
    "            print(\"Silhouette score: Not applicable (only one cluster)\\n\")\n",
    "            continue\n",
    "\n",
    "        if algorithm_name.endswith(('Gow-Sil', 'Gow-Gap')):\n",
    "            metric = 'precomputed'\n",
    "        else:\n",
    "            metric = 'euclidean'\n",
    "\n",
    "        silhouette = silhouette_score(\n",
    "            X_distance if metric == 'precomputed' else X,\n",
    "            labels,\n",
    "            metric=metric)\n",
    "        print(f\"Silhouette score: {silhouette}\\n\")\n",
    "\n",
    "\n",
    "print_label_counts(cluster_labels, cluster_vars_encoded, gower_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de36b7f",
   "metadata": {},
   "source": [
    "## Prognostic value of clusterings and stability bootstrap in scenario 1 \n",
    "<a class=\"anchor\" id=\"chapter9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c06f71ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  auc_apparent\n",
      "0    KM-Gow-Sil      0.539362\n",
      "1   KM-Eucl-Sil      0.554277\n",
      "2    AC-Gow-Sil      0.500599\n",
      "3   AC-Eucl-Sil      0.534569\n",
      "4    SC-Gow-Sil      0.642875\n",
      "5   SC-Eucl-Sil      0.553106\n",
      "6    KM-Gow-Gap      0.533356\n",
      "7   KM-Eucl-Gap      0.565788\n",
      "8    AC-Gow-Gap      0.526520\n",
      "9   AC-Eucl-Gap      0.526146\n",
      "10   SC-Gow-Gap      0.528737\n",
      "11  SC-Eucl-Gap      0.554852\n",
      "\n",
      "1\n",
      "An error occurred: Incompatible dimension for X and Y matrices: X.shape[1] == 15 while Y.shape[1] == 12\n",
      "Algorithm: KMedoids \n",
      "Optimism Average: 0.051838788173803385 \n",
      "Std.dev. Bootstrap AUCs: 0.0 \n",
      "\n",
      "1\n",
      "An error occurred: Incompatible dimension for X and Y matrices: X.shape[1] == 15 while Y.shape[1] == 12\n",
      "Algorithm: KMedoids \n",
      "Optimism Average: 0.05543513240627676 \n",
      "Std.dev. Bootstrap AUCs: 0.0037064600106003764 \n",
      "\n",
      "1\n",
      "An error occurred: Incompatible dimension for X and Y matrices: X.shape[1] == 15 while Y.shape[1] == 12\n",
      "Algorithm: KMedoids \n",
      "Optimism Average: 0.05243199577673443 \n",
      "Std.dev. Bootstrap AUCs: 0.0037117997070475455 \n",
      "\n",
      "Rand             0.987384\n",
      "Adjusted Rand    0.974766\n",
      "dtype: float64\n",
      "None\n",
      "\n",
      "1\n",
      "Algorithm: AgglomerativeClustering \n",
      "Optimism Average: -0.015989617615219665 \n",
      "Std.dev. Bootstrap AUCs: 0.02703768258616418 \n",
      "\n",
      "1\n",
      "Algorithm: AgglomerativeClustering \n",
      "Optimism Average: 0.0013800953703526353 \n",
      "Std.dev. Bootstrap AUCs: 0.027231448285109087 \n",
      "\n",
      "1\n",
      "Algorithm: AgglomerativeClustering \n",
      "Optimism Average: 0.007307410340274638 \n",
      "Std.dev. Bootstrap AUCs: 0.025860639544126533 \n",
      "\n",
      "Rand             0.729270\n",
      "Adjusted Rand    0.333333\n",
      "dtype: float64\n",
      "None\n",
      "\n",
      "1\n",
      "Algorithm: SpectralClustering \n",
      "Optimism Average: -0.03702169345779022 \n",
      "Std.dev. Bootstrap AUCs: 0.024537917652344903 \n",
      "\n",
      "1\n",
      "Algorithm: SpectralClustering \n",
      "Optimism Average: 0.0032567836516418786 \n",
      "Std.dev. Bootstrap AUCs: 0.02299401380059195 \n",
      "\n",
      "1\n",
      "Algorithm: SpectralClustering \n",
      "Optimism Average: 0.011208223047169896 \n",
      "Std.dev. Bootstrap AUCs: 0.02193833215378575 \n",
      "\n",
      "Rand             0.562882\n",
      "Adjusted Rand    0.333333\n",
      "dtype: float64\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine target variable (y_dich) with encoded cluster variables\n",
    "orig_data = pd.concat([y_dich, cluster_vars_encoded], axis=1)\n",
    "orig_data.columns = ['y_orig'] + list(cluster_vars_encoded.columns)\n",
    "\n",
    "# Combine target variable (y_dich) with gap-encoded cluster variables\n",
    "orig_data_gap = pd.concat([y_dich, cluster_vars_gap], axis=1)\n",
    "orig_data_gap.columns = ['y_orig'] + list(cluster_vars_gap.columns)\n",
    "\n",
    "# Set bootstrap iterations, range of clusters, fixed number of clusters (k)\n",
    "n_bootstraps = 3\n",
    "range_n_clusters = range(2, 4)\n",
    "k = 4\n",
    "\n",
    "# Lists to store AUC values and cluster labels\n",
    "aucs_bootstrap = []\n",
    "clusters = []\n",
    "\n",
    "# DataFrame to store AUC Apparent values for each algorithm\n",
    "auc_apparents = pd.DataFrame(columns=['method', 'auc_apparent'])\n",
    "\n",
    "# Dictionary of clustering algorithms and their corresponding classes\n",
    "algorithms = {\n",
    "    'KMedoids': KMedoids,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'SpectralClustering': SpectralClustering\n",
    "}\n",
    "\n",
    "# Iterate through clustering algorithms and calculate AUC Apparent\n",
    "for algorithm, labels in cluster_labels.items():\n",
    "    # Create DataFrame for logistic regression with cluster labels\n",
    "    df_orig = pd.DataFrame({\n",
    "        \"y_orig\": np.ravel(y_dich),\n",
    "        \"labels_orig\": np.ravel(labels)\n",
    "    })\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = sm.formula.glm(formula=\"y_orig ~ labels_orig\",\n",
    "                           family=sm.families.Binomial(),\n",
    "                           data=df_orig).fit()\n",
    "\n",
    "    # Calculate AUC Apparent\n",
    "    y_pred_apparent = model.predict(df_orig['labels_orig'])\n",
    "    auc_apparent = roc_auc_score(df_orig['y_orig'], y_pred_apparent)\n",
    "\n",
    "    # Append results to auc_apparents DataFrame\n",
    "    auc_apparents = auc_apparents.append(\n",
    "        {\n",
    "            'method': algorithm,\n",
    "            'auc_apparent': auc_apparent\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display AUC Apparent values\n",
    "print(auc_apparents)\n",
    "print()\n",
    "\n",
    "# Iterate through clustering algorithms and perform bootstrap iterations\n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "    # Lists to store optimism values and cluster labels\n",
    "    optimisms = []\n",
    "    labels_origs = pd.DataFrame()\n",
    "\n",
    "    # Perform bootstrap iterations\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample data for bootstrap\n",
    "        sample = resample(orig_data, replace=True, n_samples=4509)\n",
    "        sample_gap = resample(orig_data_gap, replace=True, n_samples=4509)\n",
    "\n",
    "        # Extract target variable from resampled data\n",
    "        y_boot = sample['y_orig']\n",
    "\n",
    "        # Calculate the best number of clusters\n",
    "        n_eucl_sil = best_silhouette_euclidean(algorithm, sample.iloc[:, 1:],\n",
    "                                               range_n_clusters)[0]\n",
    "\n",
    "        # n_gow_sil = best_silhouette_gower(algorithm, gower.gower_matrix(sample.iloc[:, 1:]),range_n_clusters)[0]\n",
    "\n",
    "        n_eucl_gap = GapStatEucl.fit_predict(algorithm, k, sample_gap.iloc[:,1:])\n",
    "        print(n_eucl_gap)\n",
    "\n",
    "        # n_gow_gap = GapStatGow.fit_predict(algorithm, k, sample_gap.iloc[:,1:])\n",
    "\n",
    "        #\n",
    "\n",
    "        # (2) AUC Bootstrap\n",
    "        \n",
    "        for n_eucl in (n_eucl_sil, n_eucl_gap):\n",
    "\n",
    "            try:\n",
    "                if algorithm == KMedoids and n_eucl == n_eucl_sil:\n",
    "                    clusterer = KMedoids(n_clusters=n_eucl).fit(sample.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.predict(orig_data.iloc[:, 1:]))\n",
    "                elif algorithm == KMedoids and n_eucl == n_eucl_gap:\n",
    "                    clusterer = KMedoids(n_clusters=n_eucl).fit(sample_gap.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.predict(orig_data.iloc[:, 1:]))\n",
    "                    \n",
    "                elif algorithm == AgglomerativeClustering and n_eucl == n_eucl_sil:\n",
    "                    clusterer = AgglomerativeClustering(n_clusters=n_eucl, linkage='average').fit(sample.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.fit_predict(orig_data.iloc[:, 1:]))\n",
    "                elif algorithm == AgglomerativeClustering and n_eucl == n_eucl_gap:\n",
    "                    clusterer = AgglomerativeClustering(n_clusters=n_eucl, linkage='average').fit(sample_gap.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.fit_predict(orig_data.iloc[:, 1:]))\n",
    "                    \n",
    "                elif algorithm == SpectralClustering and n_eucl == n_eucl_sil:\n",
    "                    clusterer = SpectralClustering(n_clusters=n_eucl, affinity='nearest_neighbors').fit(sample.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.fit_predict(orig_data.iloc[:, 1:]))\n",
    "                elif algorithm == SpectralClustering and n_eucl == n_eucl_gap:\n",
    "                    clusterer = SpectralClustering(n_clusters=n_eucl, affinity='nearest_neighbors').fit(sample_gap.iloc[:, 1:])\n",
    "                    labels_orig = pd.DataFrame(clusterer.fit_predict(orig_data.iloc[:, 1:]))\n",
    "\n",
    "                # Get labels and fit logistic regression model\n",
    "                labels_boot = pd.DataFrame(clusterer.labels_)\n",
    "                df_boot = pd.DataFrame({\n",
    "                    \"y_boot\": np.ravel(y_boot),\n",
    "                    \"labels_boot\": np.ravel(labels_boot)\n",
    "                })\n",
    "\n",
    "                model = sm.formula.glm(formula=\"y_boot ~ labels_boot\",\n",
    "                                       family=sm.families.Binomial(),\n",
    "                                       data=df_boot).fit()\n",
    "\n",
    "                # Calculate AUC Bootstrap\n",
    "                y_pred_boot = model.predict(labels_boot)\n",
    "                auc_bootstrap = roc_auc_score(y_boot, y_pred_boot)\n",
    "                aucs_bootstrap.append(auc_bootstrap)\n",
    "\n",
    "                # Calculate AUC Original\n",
    "                y_pred_orig = model.predict(pd.DataFrame(labels_orig))\n",
    "                auc_original = roc_auc_score(df_orig['y_orig'], y_pred_orig)\n",
    "\n",
    "                # Calculate optimism and append results\n",
    "                optimism = auc_bootstrap - auc_original\n",
    "                optimisms.append(optimism)\n",
    "                labels_origs = pd.concat([labels_origs, labels_orig], axis=1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"An error occurred:\", str(e))\n",
    "                continue\n",
    "\n",
    "        # Calculate average optimism\n",
    "        optimism_avg = np.mean(optimisms)\n",
    "\n",
    "        std_aucs_bootstrap = np.std(aucs_bootstrap)\n",
    "\n",
    "        print(f\"Algorithm: {algorithm_name} \\n\"\n",
    "              f\"Optimism Average: {optimism_avg} \\n\"\n",
    "              f\"Std.dev. Bootstrap AUCs: {std_aucs_bootstrap} \\n\")\n",
    "\n",
    "    #         auc_o = auc_apparent - optimism_avg\n",
    "\n",
    "    #         ci = [\n",
    "    #             auc_o - 1.96 * np.std(aucs_bootstrap),\n",
    "    #             auc_o + 1.96 * np.std(aucs_bootstrap)\n",
    "    #         ]\n",
    "\n",
    "    # Stability\n",
    "    \n",
    "    stability = stability_bootstrap(labels_origs, n_bootstraps)\n",
    "\n",
    "    print(stability)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e75f96",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Scenario 1 adjusted Rand stability analysis via logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be54b21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'algorithm':\n",
    "    ['AC', 'KM', 'SC', 'AC', 'KM', 'SC', 'AC', 'KM', 'SC', 'AC', 'KM', 'SC'],\n",
    "    'distance': [\n",
    "        'Gow', 'Gow', 'Gow', 'Eucl', 'Eucl', 'Eucl', 'Gow', 'Gow', 'Gow',\n",
    "        'Eucl', 'Eucl', 'Eucl'\n",
    "    ],\n",
    "    'method': [\n",
    "        'Sil', 'Sil', 'Sil', 'Sil', 'Sil', 'Sil', 'Gap', 'Gap', 'Gap', 'Gap',\n",
    "        'Gap', 'Gap'\n",
    "    ],\n",
    "    'value':\n",
    "    [0.47, 0.20, 0.98, 0.60, 0.67, 0.57, 0.96, 0.34, 0.37, 0.61, 0.77, 0.66]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df,\n",
    "                            columns=['algorithm', 'distance', 'method'],\n",
    "                            prefix=['algorithm', 'distance', 'method'],\n",
    "                            drop_first=True)\n",
    "\n",
    "X = df_encoded[['algorithm_KM', 'algorithm_SC', 'distance_Gow', 'method_Sil']]\n",
    "y = df['value']\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS (Ordinary Least Squares) model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary of the logistic regression model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22d96e",
   "metadata": {},
   "source": [
    "## SHAP analysis for scenario 1 <a class=\"anchor\" id=\"chapter10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y1 = cluster_labels['AC-Gow-Sil']\n",
    "y2 = cluster_labels['KM-Gow-Sil']\n",
    "y3 = cluster_labels['SC-Gow-Sil']\n",
    "y4 = cluster_labels['AC-Eucl-Sil']\n",
    "y5 = cluster_labels['KM-Eucl-Sil']\n",
    "y6 = cluster_labels['SC-Eucl-Sil']\n",
    "y7 = cluster_labels['AC-Gow-Gap']\n",
    "y8 = cluster_labels['KM-Gow-Gap']\n",
    "y9 = cluster_labels['SC-Gow-Gap']\n",
    "y10 = cluster_labels['AC-Eucl-Gap']\n",
    "y11 = cluster_labels['KM-Eucl-Gap']\n",
    "y12 = cluster_labels['SC-Eucl-Gap']\n",
    "\n",
    "cluster_labels_list = [(y1, 'AC-Gow-Sil'), (y2, 'KM-Gow-Sil'),\n",
    "                       (y3, 'SC-Gow-Sil'), (y4, 'AC-Eucl-Sil'),\n",
    "                       (y5, 'KM-Eucl-Sil'), (y6, 'SC-Eucl-Sil'),\n",
    "                       (y7, 'AC-Gow-Gap'), (y8, 'KM-Gow-Gap'),\n",
    "                       (y9, 'SC-Gow-Gap'), (y10, 'AC-Eucl-Gap'),\n",
    "                       (y11, 'KM-Eucl-Gap'), (y12, 'SC-Eucl-Gap')]\n",
    "\n",
    "\n",
    "def shap_feature_ranking(data, shap_values, columns=None):\n",
    "    if columns is None:\n",
    "        columns = data.columns.tolist()\n",
    "\n",
    "    # Get column indices for the specified columns\n",
    "    c_idxs = [data.columns.get_loc(column) for column in columns]\n",
    "\n",
    "    # Calculate mean shap values\n",
    "    if isinstance(shap_values, list):\n",
    "        means = [\n",
    "            np.abs(shap_values[class_][:, c_idxs]).mean(axis=0)\n",
    "            for class_ in range(len(shap_values))\n",
    "        ]\n",
    "        shap_means = np.sum(np.column_stack(means), axis=1)\n",
    "    else:\n",
    "        assert len(shap_values.shape) == 2, 'Expected two-dimensional shap values array.'\n",
    "        shap_means = np.abs(shap_values[:, c_idxs]).mean(axis=0)\n",
    "\n",
    "    df_ranking = pd.DataFrame({\n",
    "        'feature': columns,\n",
    "        'mean_shap_value': shap_means\n",
    "    })\n",
    "\n",
    "    df_ranking = df_ranking.sort_values(by='mean_shap_value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df_ranking.index += 1\n",
    "\n",
    "    return df_ranking\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Perform SHAP analysis for each cluster label\n",
    "for i, (cluster_label, label_name) in enumerate(cluster_labels_list, start=1):\n",
    "    # Create and fit the RandomForestClassifier\n",
    "    clf.fit(cluster_vars_encoded, cluster_label)\n",
    "\n",
    "    # Create a TreeExplainer\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "\n",
    "    # Calculate SHAP values for all samples\n",
    "    shap_values = explainer.shap_values(cluster_vars_encoded)\n",
    "\n",
    "    # Get the feature ranking\n",
    "    feature_ranking = shap_feature_ranking(cluster_vars_encoded, shap_values)\n",
    "\n",
    "    # Align feature order based on the first model's order\n",
    "    if i == 1:\n",
    "        feature_order = feature_ranking['feature'].tolist()\n",
    "    else:\n",
    "        feature_ranking = feature_ranking.set_index('feature').reindex(feature_order).reset_index()\n",
    "\n",
    "    # Print the model name\n",
    "    print(f'Model: {label_name}')\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(feature_ranking)\n",
    "    print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d6ba8",
   "metadata": {},
   "source": [
    "## GridSearch Gaussian Mixture <a class=\"anchor\" id=\"chapter11\"></a>\n",
    " \n",
    "Sklearn's default value for the number of components in the Gaussian Mixture clustering algorithm is 1. Therefore, we performed a grid search to find an alternative number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdd445",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(1, 35),\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV 20 times\n",
    "for i in range(20):\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=GaussianMixture(),\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=gmm_bic_score)\n",
    "\n",
    "    # Fit the model using scaled encoded cluster variables\n",
    "    grid_search.fit(cluster_vars_encoded_scaled)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(grid_search.cv_results_,\n",
    "                      columns=[\n",
    "                          \"param_n_components\", \"param_covariance_type\",\n",
    "                          \"mean_test_score\"\n",
    "                      ])\n",
    "\n",
    "    # Calculate BIC score and update DataFrame\n",
    "    df[\"BIC score\"] = -df[\"mean_test_score\"]\n",
    "    df.drop('mean_test_score', axis=1, inplace=True)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        \"param_n_components\": \"Number of components\",\n",
    "        \"param_covariance_type\": \"Type of covariance\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Sort DataFrame by BIC score\n",
    "    df.sort_values(by=\"BIC score\", inplace=True)\n",
    "\n",
    "    # Display the top rows of the DataFrame\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c653f2b",
   "metadata": {},
   "source": [
    "## Scenario 2 <a class=\"anchor\" id=\"chapter12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94677eeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_algorithms():\n",
    "    kmeans = KMeans()\n",
    "    kmedoids = KMedoids()\n",
    "    gmm = GaussianMixture(n_components=45, covariance_type='diag')\n",
    "    average_linkage = AgglomerativeClustering(linkage='average')\n",
    "    ward = AgglomerativeClustering(linkage='ward')\n",
    "    spectral = SpectralClustering(affinity='nearest_neighbors')\n",
    "    birch = Birch()\n",
    "    mean_shift = MeanShift()\n",
    "    affinity_propagation = AffinityPropagation()\n",
    "    dbscan = DBSCAN()\n",
    "    optics = OPTICS()\n",
    "\n",
    "    algorithms = [('KMeans', kmeans), ('KMedoids', kmedoids),\n",
    "                  ('GaussianMixture', gmm),\n",
    "                  ('AgglomerativeClustering', average_linkage), ('Ward', ward),\n",
    "                  ('SpectralClustering', spectral), ('BIRCH', birch),\n",
    "                  ('Mean Shift', mean_shift),\n",
    "                  ('AffinityPropagation', affinity_propagation),\n",
    "                  ('DBSCAN', dbscan), ('OPTICS', optics)]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 4,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"random_state\": 42,\n",
    "    \"metric\": 'precomputed'\n",
    "}\n",
    "\n",
    "clustering_algorithms = prepare_algorithms()\n",
    "\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = {}\n",
    "\n",
    "    for algorithm_name, algorithm in clustering_algorithms:\n",
    "        try:\n",
    "            algorithm.fit(data)\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels[algorithm_name] = algorithm.labels_.astype(int)\n",
    "            elif hasattr(algorithm, 'predict'):\n",
    "                labels[algorithm_name] = algorithm.predict(data)\n",
    "            else:\n",
    "                raise AttributeError(\n",
    "                    f\"{algorithm_name} does not have a labels_ attribute or a predict method.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {algorithm_name} could not be fitted. {e}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "cluster_labels = get_labels(cluster_vars_encoded_scaled)\n",
    "\n",
    "\n",
    "def print_label_counts(cluster_labels, X):\n",
    "    if not cluster_labels:\n",
    "        print(\"Error: Input is empty.\")\n",
    "        return\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        label_counts = Counter(labels)\n",
    "        cluster_sizes = list(label_counts.values())\n",
    "\n",
    "        print(f\"Algorithm: {algorithm_name}\")\n",
    "        print(f\"Number of unique labels: {len(label_counts)}\")\n",
    "        print(f\"Largest: {max(cluster_sizes)}\")\n",
    "        print(f\"Smallest: {min(cluster_sizes)}\")\n",
    "        print(f\"Average: {np.mean(cluster_sizes)}\")\n",
    "        print(f\"Median: {np.median(cluster_sizes)}\")\n",
    "\n",
    "        if len(label_counts) == 1:\n",
    "            print(\"Silhouette score: Not applicable (only one cluster)\\n\")\n",
    "        else:\n",
    "            silhouette = silhouette_score(X, labels, metric='euclidean')\n",
    "            print(f\"Silhouette score: {silhouette}\\n\")\n",
    "\n",
    "\n",
    "print_label_counts(cluster_labels, cluster_vars_encoded_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b75699",
   "metadata": {},
   "source": [
    "## Print the top ten largest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9118e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store label counts for each method\n",
    "method_label_counts = {}\n",
    "\n",
    "# Iterate through each method in the dictionary\n",
    "for method, labels in cluster_labels.items():\n",
    "    # Count the occurrences of each label for the current method\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    # Store the label counts in the dictionary\n",
    "    method_label_counts[method] = label_counts\n",
    "\n",
    "# Print the top 10 label counts for each method\n",
    "for method, label_counts in method_label_counts.items():\n",
    "    print(f\"{method} Top 10 Label Counts:\")\n",
    "    for label, count in label_counts.most_common(10):\n",
    "        print(f\"Label: {label}, Count: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb5fbd",
   "metadata": {},
   "source": [
    "## SHAP analysis for scenario 2 <a class=\"anchor\" id=\"chapter13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a872f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def shap_feature_ranking(data, shap_values, columns=[]):\n",
    "    if not columns: columns = data.columns.tolist()\n",
    "\n",
    "    c_idxs = []\n",
    "    for column in columns:\n",
    "        c_idxs.append(data.columns.get_loc(column))\n",
    "    if isinstance(shap_values, list):\n",
    "        means = [\n",
    "            np.abs(shap_values[class_][:, c_idxs]).mean(axis=0)\n",
    "            for class_ in range(len(shap_values))\n",
    "        ]\n",
    "        shap_means = np.sum(np.column_stack(means), 1)\n",
    "    else:\n",
    "        assert len(shap_values.shape\n",
    "                   ) == 2, 'Expected two-dimensional shap values array.'\n",
    "        shap_means = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    df_ranking = pd.DataFrame({\n",
    "        'feature': columns,\n",
    "        'mean_shap_value': shap_means\n",
    "    }).sort_values(by='mean_shap_value',\n",
    "                   ascending=False).reset_index(drop=True)\n",
    "    df_ranking.index += 1\n",
    "\n",
    "    return df_ranking\n",
    "\n",
    "\n",
    "y1 = cluster_labels['AffinityPropagation']\n",
    "y2 = cluster_labels['BIRCH']\n",
    "y3 = cluster_labels['AgglomerativeClustering']\n",
    "y4 = cluster_labels['DBSCAN']\n",
    "y5 = cluster_labels['GaussianMixture']\n",
    "y6 = cluster_labels['Ward']\n",
    "y7 = cluster_labels['KMeans']\n",
    "y8 = cluster_labels['KMedoids']\n",
    "y9 = cluster_labels['Mean Shift']\n",
    "y10 = cluster_labels['SpectralClustering']\n",
    "y11 = cluster_labels['OPTICS']\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "cluster_labels_list = [(y1, 'AffinityPropagation'), (y2, 'BIRCH'),\n",
    "                       (y3, 'AgglomerativeClustering'), (y4, 'DBSCAN'),\n",
    "                       (y5, 'GaussianMixture'), (y6, 'Ward'), (y7, 'KMeans'),\n",
    "                       (y8, 'KMedoids'), (y9, 'MeanShift'),\n",
    "                       (y10, 'SpectralClustering'), (y11, 'OPTICS')]\n",
    "\n",
    "all_models = pd.DataFrame()\n",
    "\n",
    "# Perform SHAP analysis for each cluster label\n",
    "for i, (cluster_label, label_name) in enumerate(cluster_labels_list, start=1):\n",
    "    # Create and fit the RandomForestClassifier\n",
    "    clf.fit(cluster_vars_encoded_scaled, cluster_label)\n",
    "\n",
    "    # Create a TreeExplainer\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "\n",
    "    # Calculate SHAP values for all samples\n",
    "    shap_values = explainer.shap_values(cluster_vars_encoded_scaled)\n",
    "\n",
    "    # Get the feature ranking\n",
    "    feature_ranking = shap_feature_ranking(cluster_vars_encoded_scaled,\n",
    "                                           shap_values)\n",
    "\n",
    "    # Align feature order based on the first model's order\n",
    "    if i == 1:\n",
    "        feature_order = feature_ranking['feature'].tolist()\n",
    "    else:\n",
    "        feature_ranking = feature_ranking.set_index('feature').reindex(\n",
    "            feature_order).reset_index()\n",
    "\n",
    "    # Append the feature ranking to the final dataframe\n",
    "    all_models = pd.concat([all_models, feature_ranking], ignore_index=True)\n",
    "\n",
    "    # Print the model name\n",
    "    print(f'Model: {label_name}')\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(feature_ranking)\n",
    "    print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bab549",
   "metadata": {},
   "source": [
    "## Prognostic value of clusterings and stability bootstrap in scenario 2 \n",
    "<a class=\"anchor\" id=\"chapter14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27252a8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_data = pd.concat([y_dich, cluster_vars_encoded_scaled], axis=1)\n",
    "orig_data.columns = ['y_orig'] + list(cluster_vars_encoded_scaled.columns)\n",
    "\n",
    "# (1) AUC Apparent\n",
    "aucs_apparent = {}\n",
    "\n",
    "for algorithm_name, labels in cluster_labels.items():\n",
    "    df = pd.DataFrame({\"y_orig\": np.ravel(y_dich), \"labels\": np.ravel(labels)})\n",
    "    formula = \"y_orig ~ labels\"\n",
    "    model = sm.formula.glm(formula=formula, family=sm.families.Binomial(), data=df).fit()\n",
    "    y_pred_apparent = model.predict(df['labels'])\n",
    "    \n",
    "    print(f\"Algorithm: {algorithm_name}, AUC Apparent: {auc_apparent}\", end='\\n')\n",
    "    \n",
    "    aucs_apparent[algorithm_name] = auc_apparent\n",
    "    \n",
    "for algorithm_name, algorithm in clustering_algorithms:\n",
    "    \n",
    "    n_bootstraps = 5\n",
    "    aucs_bootstrap = []\n",
    "    optimisms = []\n",
    "    labels_origs = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        sample = resample(orig_data, replace=True, n_samples=4509)\n",
    "\n",
    "        y_boot = sample.iloc[:,0]\n",
    "        \n",
    "        # (2) AUC Bootstrap\n",
    "        try:\n",
    "            algorithm.fit(sample.iloc[:,1:])\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                labels_boot = pd.DataFrame(algorithm.labels_)\n",
    "            elif hasattr(algorithm, 'predict'):\n",
    "                labels_boot = pd.DataFrame(algorithm.predict(sample.iloc[:,1:]))\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "            continue\n",
    "        \n",
    "        boot_data = pd.DataFrame({\"y_boot\": np.ravel(y_boot), \"labels_boot\": np.ravel(labels_boot)})\n",
    "\n",
    "        model = sm.formula.glm(formula=\"y_boot ~ labels_boot\", family=sm.families.Binomial(), data=boot_data).fit()\n",
    "\n",
    "        y_pred_boot = model.predict(labels_boot) # Probabilties between 0 / 1\n",
    "\n",
    "        auc_bootstrap = roc_auc_score(y_boot, y_pred_boot)\n",
    "        \n",
    "        aucs_bootstrap.append(auc_bootstrap)\n",
    "        \n",
    "        # (3) AUC Original \n",
    "        if isinstance(algorithm, (AgglomerativeClustering, SpectralClustering, MeanShift, DBSCAN, OPTICS)):\n",
    "            labels_orig = pd.DataFrame(algorithm.fit_predict(orig_data.iloc[:,1:]))\n",
    "        else:\n",
    "            labels_orig = pd.DataFrame(algorithm.predict(orig_data.iloc[:,1:]))\n",
    "\n",
    "        y_pred_orig = model.predict(labels_orig)\n",
    "\n",
    "        auc_original = roc_auc_score(df['y_orig'], y_pred_orig)\n",
    "        \n",
    "        optimism = auc_bootstrap - auc_original\n",
    "\n",
    "        optimisms.append(optimism)\n",
    "        \n",
    "        #\n",
    "        \n",
    "        labels_origs = pd.concat([labels_origs, labels_orig], axis=1)\n",
    "     \n",
    "    optimism_avg = np.mean(optimisms) # Should be positive\n",
    "    \n",
    "    aucs_o = aucs_apparent[algorithm_name] - optimism_avg\n",
    "    \n",
    "    ci = [aucs_o - 1.96 * np.std(aucs_bootstrap), aucs_o + 1.96 * np.std(aucs_bootstrap)]\n",
    "        \n",
    "    print(f\"Algorithm: {algorithm_name}\\n\"\n",
    "      f\"Optimism Average: {optimism_avg}\\n\"\n",
    "      f\"AUC Apparent: {aucs_apparent[algorithm_name]}\\n\"\n",
    "      f\"AUC Optimism Adjusted: {aucs_o}\\n\"\n",
    "      f\"Confidence Interval: {ci}\")\n",
    "\n",
    "    # Stability\n",
    "    \n",
    "    stability_bootstrap(labels_origs, n_bootstraps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe837f",
   "metadata": {},
   "source": [
    "##  Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_vectors(c):\n",
    "    v = [[], []]\n",
    "\n",
    "    for i, row in enumerate(c):\n",
    "        for j, val in enumerate(row):\n",
    "            v[0].extend([i] * val)\n",
    "            v[1].extend([j] * val)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc831feb",
   "metadata": {},
   "source": [
    "## Agreement via the pair-confusion matrix <a class=\"anchor\" id=\"chapter15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d00fcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def agreement_pairconf(cluster_labels):\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "    names = []\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    cluster_labels = pd.DataFrame.from_dict(cluster_labels)\n",
    "\n",
    "    # Get all pairwise combinations of the columns\n",
    "    pairwise_combinations = pd.DataFrame(\n",
    "        list(combinations(cluster_labels.columns, 2)))\n",
    "\n",
    "    # Iterate over all pairwise combinations\n",
    "    for i in range(len(pairwise_combinations)):\n",
    "\n",
    "        pairconf = pair_confusion_matrix(\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 0]],\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 1]])\n",
    "\n",
    "        a = pairconf[1, 1]  # pairs grouped together in both\n",
    "        b = pairconf[1, 0]\n",
    "        c = pairconf[0, 1]\n",
    "        d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "        rand = (a + d) / (a + d + b + c)\n",
    "        arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) * (d + c))\n",
    "\n",
    "        rands.append(rand)\n",
    "        arands.append(arand)\n",
    "\n",
    "        names.append(\n",
    "            f\"{pairwise_combinations.iloc[i,0]}_{pairwise_combinations.iloc[i,1]}\"\n",
    "        )\n",
    "\n",
    "    scores = pd.DataFrame({\n",
    "        'Rand': rands,\n",
    "        'Adjusted Rand': arands\n",
    "    },\n",
    "                          index=names)\n",
    "\n",
    "    return scores.sort_index(\n",
    "    )  # scores.sort_values('Adjusted Rand', ascending=False)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "agreement_pairconf(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d1e3e",
   "metadata": {},
   "source": [
    "## Agreement via the contingency table <a class=\"anchor\" id=\"chapter16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212bf0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def agreement_contingency(cluster_labels):\n",
    "    scores = {}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    cluster_labels = pd.DataFrame.from_dict(cluster_labels)\n",
    "\n",
    "    # Get all pairwise combinations of the columns\n",
    "    pairwise_combinations = pd.DataFrame(\n",
    "        list(combinations(cluster_labels.columns, 2)))\n",
    "\n",
    "    # Iterate over all pairwise combinations\n",
    "    for i in range(len(pairwise_combinations)):\n",
    "        contingency = contingency_matrix(\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 0]],\n",
    "            cluster_labels[pairwise_combinations.iloc[i, 1]])\n",
    "\n",
    "        mi = mutual_info_score(_, _, contingency=contingency)\n",
    "        ami = adjusted_mutual_info_score(\n",
    "            produce_vectors(contingency)[0],\n",
    "            produce_vectors(contingency)[1])\n",
    "\n",
    "        scores[pairwise_combinations.iloc[i, 0] + '_' +\n",
    "               pairwise_combinations.iloc[i, 1]] = {\n",
    "                   'Mutual Information': mi,\n",
    "                   'Adjusted Mutual Information': ami\n",
    "               }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    scores = pd.DataFrame.from_dict(scores, orient='index')\n",
    "\n",
    "    return scores.sort_index(\n",
    "    )  # scores.sort_values('Adjusted Mutual Information', ascending=False)\n",
    "\n",
    "\n",
    "agreement_contingency(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd49de8",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Input for the UpSet plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e534b1",
   "metadata": {
    "scrolled": false,
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def binary_matrix(cluster_labels):\n",
    "\n",
    "    binary = []\n",
    "    data_dict = {}\n",
    "\n",
    "    for algorithm_name, labels in cluster_labels.items():\n",
    "        n = len(labels)\n",
    "        labels_repeated = labels.repeat(n).reshape(n, n)\n",
    "        labels_transposed = labels_repeated.transpose()\n",
    "        temp = (labels_repeated == labels_transposed).astype(int)\n",
    "        col = temp[np.triu_indices(labels_repeated.shape[0], k=1)].transpose()\n",
    "        binary.append(algorithm_name)\n",
    "        binary.append(col)\n",
    "\n",
    "    for i, item in enumerate(binary):\n",
    "        if i % 2 == 0:\n",
    "            # The item is a column name\n",
    "            col_name = item\n",
    "        else:\n",
    "            data_dict[col_name] = item\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "binary_matrix(cluster_labels)\n",
    "\n",
    "#\n",
    "\n",
    "N = len(next(iter(cluster_labels.values())))\n",
    "K = len(cluster_labels)\n",
    "\n",
    "# Convert labels dictionary to numpy array\n",
    "A = np.zeros((N, K))\n",
    "for i, label_list in enumerate(cluster_labels.values()):\n",
    "    A[:, i] = label_list\n",
    "\n",
    "# Create an array to store the group results for each algorithm\n",
    "group_results = []\n",
    "\n",
    "# Iterate over the algorithms\n",
    "for i, label_list in enumerate(cluster_labels.values()):\n",
    "    # Create an empty array to store the group results for the pairs\n",
    "    algorithm_group = np.zeros(N * (N - 1) // 2, dtype=int)\n",
    "    idx = 0\n",
    "    # Iterate over the data point pairs\n",
    "    for j in range(N):\n",
    "        for k in range(j + 1, N):\n",
    "            # Check if the algorithm groups the pair or not\n",
    "            if label_list[j] == label_list[k]:\n",
    "                algorithm_group[idx] = 1\n",
    "            idx += 1\n",
    "    group_results.append(algorithm_group)\n",
    "\n",
    "# Create a Pandas DataFrame with the group results\n",
    "df = pd.DataFrame(np.column_stack(group_results),\n",
    "                  columns=cluster_labels.keys())\n",
    "\n",
    "#\n",
    "\n",
    "# Convert pandas dataframe to R dataframe\n",
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    r_df = robjects.conversion.py2rpy(df)\n",
    "\n",
    "# Save R dataframe as .rds file\n",
    "r_file = \"M1_BinaryMatrix.rds\"\n",
    "robjects.r.assign(\"my_df_tosave\", r_df)\n",
    "robjects.r(f\"saveRDS(my_df_tosave, file='{r_file}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdf1d2",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Clustering stability via the pair-confusion matrix for Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1e74c",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def stability_pairconf(data, algorithm, n_samples, n_iter=200):\n",
    "\n",
    "    labels = []\n",
    "    intersect = []\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for x in range(n_iter):\n",
    "        boot = resample(data, replace=True, n_samples=n_samples)\n",
    "        algorithm = algorithm.fit(boot)\n",
    "        labels.append(boot.index)\n",
    "\n",
    "        if isinstance(algorithm, GaussianMixture):\n",
    "            labels.append(algorithm.predict(boot))\n",
    "        else:\n",
    "            labels.append(algorithm.labels_)\n",
    "\n",
    "    labels = np.array(labels).transpose()\n",
    "    labels = pd.DataFrame(labels)\n",
    "\n",
    "    for col_pos_1 in range(n_iter):\n",
    "        for col_pos_2 in range(col_pos_1 + 1, n_iter):\n",
    "            if col_pos_1 == col_pos_2:\n",
    "                continue\n",
    "\n",
    "            intersect = list(\n",
    "                set(labels[col_pos_1 * 2]) & set(labels[col_pos_2 * 2]))\n",
    "\n",
    "            run_1_pred = labels[[col_pos_1 * 2, col_pos_1 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_1 * 2].isin(intersect)]\n",
    "            run_2_pred = labels[[col_pos_2 * 2, col_pos_2 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_2 * 2].isin(intersect)]\n",
    "\n",
    "            run_1_pred = run_1_pred.sort_values(\n",
    "                by=col_pos_1 * 2).drop_duplicates(subset=[col_pos_1 * 2])\n",
    "            run_2_pred = run_2_pred.sort_values(\n",
    "                by=col_pos_2 * 2).drop_duplicates(subset=[col_pos_2 * 2])\n",
    "\n",
    "            run_1_pred = run_1_pred[col_pos_1 * 2 + 1]\n",
    "            run_2_pred = run_2_pred[col_pos_2 * 2 + 1]\n",
    "\n",
    "            pairconf = pair_confusion_matrix(run_1_pred, run_2_pred)\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) *\n",
    "                                           (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each algorithm in clustering_algorithms\n",
    "for algorithm, algorithm_function in [\n",
    "        clustering_algorithms[3], clustering_algorithms[4],\n",
    "        clustering_algorithms[5], clustering_algorithms[7],\n",
    "        clustering_algorithms[9], clustering_algorithms[10]\n",
    "]:\n",
    "    print(algorithm_function)\n",
    "\n",
    "    # Call the stability_pairconf function with the appropriate parameters\n",
    "    result = stability_pairconf(cluster_vars_encoded_scaled,\n",
    "                                algorithm_function,\n",
    "                                n_samples=4509,\n",
    "                                n_iter=200)\n",
    "\n",
    "    # Assign a name to the row based on the algorithm\n",
    "    result.name = algorithm\n",
    "\n",
    "    # Append the result to the DataFrame\n",
    "    results_df = results_df.append(result)\n",
    "\n",
    "    # Print the intermediate result\n",
    "    print(f\"Algorithm: {algorithm}\")\n",
    "    print(result)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(results_df)\n",
    "\n",
    "stability_pairconf(cluster_vars_encoded_scaled,\n",
    "                   OPTICS(),\n",
    "                   n_samples=4509,\n",
    "                   n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b174194",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "source": [
    "## Clustering stability via the pair-confusion matrix for Gower's distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bc535",
   "metadata": {
    "tags": [
     "no"
    ]
   },
   "outputs": [],
   "source": [
    "def stability_pairconf(data, algorithm, n_samples, n_iter):\n",
    "\n",
    "    labels = []\n",
    "    intersect = []\n",
    "\n",
    "    rands = []\n",
    "    arands = []\n",
    "\n",
    "    for x in range(n_iter):\n",
    "        boot = resample(data, replace=True, n_samples=n_samples)\n",
    "        labels.append(boot.index)\n",
    "        gowerdist = pd.DataFrame(gower.gower_matrix(boot))\n",
    "\n",
    "        if isinstance(algorithm, sklearn.cluster._spectral.SpectralClustering):\n",
    "            gowersim = 1 - gowerdist\n",
    "            algorithm = algorithm.fit(gowersim)\n",
    "            labels.append(algorithm.labels_)\n",
    "        else:\n",
    "            algorithm = algorithm.fit(gowerdist)\n",
    "            labels.append(algorithm.labels_)\n",
    "\n",
    "    labels = pd.DataFrame(np.array(labels).transpose())\n",
    "    print(labels)\n",
    "\n",
    "    for col_pos_1 in range(n_iter):\n",
    "        for col_pos_2 in range(col_pos_1 + 1, n_iter):\n",
    "            if col_pos_1 == col_pos_2:\n",
    "                continue\n",
    "\n",
    "            intersect = list(\n",
    "                set(labels[col_pos_1 * 2]) & set(labels[col_pos_2 * 2]))\n",
    "\n",
    "            run_1_pred = labels[[col_pos_1 * 2, col_pos_1 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_1 * 2].isin(intersect)]\n",
    "            run_2_pred = labels[[col_pos_2 * 2, col_pos_2 * 2 + 1\n",
    "                                 ]].loc[labels[col_pos_2 * 2].isin(intersect)]\n",
    "\n",
    "            run_1_pred = run_1_pred.sort_values(\n",
    "                by=col_pos_1 * 2).drop_duplicates(subset=[col_pos_1 * 2])\n",
    "            run_2_pred = run_2_pred.sort_values(\n",
    "                by=col_pos_2 * 2).drop_duplicates(subset=[col_pos_2 * 2])\n",
    "\n",
    "            run_1_pred = run_1_pred[col_pos_1 * 2 + 1]\n",
    "            run_2_pred = run_2_pred[col_pos_2 * 2 + 1]\n",
    "\n",
    "            pairconf = pair_confusion_matrix(run_1_pred, run_2_pred)\n",
    "\n",
    "            a = pairconf[1, 1]  # pairs grouped together in both\n",
    "            b = pairconf[1, 0]\n",
    "            c = pairconf[0, 1]\n",
    "            d = pairconf[0, 0]  # pairs not grouped together in both\n",
    "\n",
    "            rand = (a + d) / (a + d + b + c)\n",
    "            arand = 2 * (a * d - b * c) / ((a + b) * (d + b) + (a + c) *\n",
    "                                           (d + c))\n",
    "\n",
    "            rands.append(rand)\n",
    "            arands.append(arand)\n",
    "\n",
    "    scores = pd.DataFrame({'Rand': rands, 'Adjusted Rand': arands})\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for algorithm, algorithm_function in [\n",
    "        clustering_algorithms[6], clustering_algorithms[8],\n",
    "        clustering_algorithms[10]\n",
    "]:\n",
    "    print(algorithm_function)\n",
    "\n",
    "    result = stability_pairconf(cluster_vars_encoded,\n",
    "                                algorithm_function,\n",
    "                                n_samples=4509,\n",
    "                                n_iter=200)\n",
    "\n",
    "    result.name = algorithm\n",
    "\n",
    "    results_df = results_df.append(result)\n",
    "\n",
    "    print(f\"Algorithm: {algorithm}\")\n",
    "    print(result)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
